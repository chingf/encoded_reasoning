{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d8b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92229513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import storage_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465ff8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "model_storage_dir = os.path.join(storage_dir, \"lm_sys\", model_name.split(\"/\")[-1])\n",
    "response_path = os.path.join(model_storage_dir, 'lm_sys_responses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c13e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversation'],\n",
      "    num_rows: 256\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(response_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc8bfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation': [{'content': 'how can identity protection services help protect me against identity theft',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Identity protection services can play a crucial role in helping you protect yourself against identity theft by monitoring, detecting, and responding to potential threats. Here's how they can help:\\n\\n---\\n\\n### **1. Continuous Monitoring of Your Personal Information**\\n- **Credit Reports:** Many services monitor your credit reports for suspicious activity, such as new accounts or loans opened in your name.\\n- **Dark Web Monitoring:** They scan the dark web (hidden parts of the internet) for your personal information, like Social Security numbers, credit card details, or login credentials.\\n- **Public Records:** They check public records for any unauthorized use of your name, address, or other personal details.\\n\\n---\\n\\n### **2. Early Detection of Fraud**\\n- **Unusual Activity Alerts:** If someone tries to open a new account, apply for a loan, or make a purchase using your identity, the service can alert you immediately.\\n- **Credit Score Monitoring:** Some services track changes in your credit score, which can be an early sign of identity theft.\\n\\n---\\n\\n### **3. Identity Theft Response and Recovery**\\n- **Fraud Alerts:** If identity theft is detected, the service can help you place fraud alerts on your credit reports.\\n- **Credit Freeze Assistance:** They may guide you through freezing your credit to prevent new accounts from being opened in your name.\\n- **Identity Theft Reports:** They can help you file an official report with the Federal Trade Commission (FTC) or other relevant authorities.\\n- **Recovery Support:** Some services offer step-by-step guidance or even direct assistance in recovering your identity and restoring your credit.\\n\\n---\\n\\n### **4. Identity Theft Insurance**\\n- Some services include **identity theft insurance** that covers costs related to fraud, such as legal fees, lost wages, or expenses incurred during recovery.\\n\\n---\\n\\n### **5. Secure Storage of Sensitive Information**\\n- Many identity protection services offer **secure digital vaults** where you can store sensitive documents (e.g., Social Security cards, passports, bank statements) safely.\\n\\n---\\n\\n### **6. Education and Awareness**\\n- These services often provide **educational resources** to help you recognize phishing attempts, scams, and other common identity theft tactics.\\n\\n---\\n\\n### **Common Features of Identity Protection Services:**\\n| Feature | Description |\\n|--------|-------------|\\n| Credit Monitoring | Tracks changes in your credit report. |\\n| Dark Web Monitoring | Scans for your personal data on the dark web. |\\n| Fraud Alerts | Notifies you of suspicious activity. |\\n| Identity Theft Insurance | Covers costs related to identity theft. |\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2da7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation': [{'content': \"Beside OFAC's selective sanction that target the listed individiuals and entities, please elaborate on the other types of US's sanctions, for example, comprehensive and sectoral sanctions. Please be detailed as much as possible\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'The U.S. sanctions regime is a multifaceted and comprehensive tool used to advance foreign policy and national security objectives. While the **Office of Foreign Assets Control (OFAC)** within the U.S. Department of the Treasury is primarily responsible for enforcing **targeted sanctions** (such as **individual and entity-specific sanctions**), the U.S. government also employs a range of **other types of sanctions**, including **comprehensive sanctions**, **sectoral sanctions**, **embargoes**, and **economic sanctions**. These are typically imposed by the **Department of the Treasury**, the **Department of State**, and the **Department of Commerce**, and are often authorized under various **statutes** such as the **International Emergency Economic Powers Act (IEEPA)**, the **Trading with the Enemy Act (TWEA)**, and the **Iran Threat Reduction and Syria Human Rights Act**.\\n\\nBelow is a detailed explanation of the **other types of U.S. sanctions**, beyond the **targeted sanctions** enforced by OFAC:\\n\\n---\\n\\n## 1. **Comprehensive Sanctions**\\n\\n**Definition**: Comprehensive sanctions are broad-based economic and trade restrictions that apply to an entire country or a large portion of its economy. These sanctions are typically imposed when the U.S. seeks to exert significant pressure on a country as a whole, rather than on specific individuals or entities.\\n\\n### Key Characteristics:\\n- **Apply to all sectors** of the economy.\\n- **Prohibit or restrict trade, investment, and financial transactions** with the sanctioned country.\\n- **May include embargoes** on goods, services, and technology.\\n- **Can be imposed for a wide range of reasons**, including human rights abuses, proliferation of weapons of mass destruction (WMDs), support for terrorism, or violations of international law.\\n\\n### Examples:\\n- **Iran**: The U.S. has imposed comprehensive sanctions on Iran since the 1980s, including restrictions on oil exports, financial transactions, and trade. These were significantly expanded under the **Iran Nuclear Deal (JCPOA)** and later re-imposed after the U.S. withdrew in 2018.\\n- **North Korea**: The U.S. has imposed comprehensive sanctions on North Korea, including restrictions on trade, financial transactions, and access to international markets, due to its nuclear program and human rights violations.\\n- **Russia**: Following the 2014 annexation of Crimea and the 2022 invasion of Ukraine, the U.S. imposed comprehensive sanctions on Russia, including',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88f0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e58fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90467719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_storage_dir):\n",
    "    os.makedirs(model_storage_dir)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", \n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5f609",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d362982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"lmsys/lmsys-chat-1m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f84c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b751aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation_for_qwen(user_prompt):\n",
    "    \"\"\"Format conversation for Qwen3-30B-A3B input with enable_thinking=False\"\"\"\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        user_prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # <-- Set this flag\n",
    "    )\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7665b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_batch(dataset, batch_size=1, max_samples=None, context_length=4096):\n",
    "    \"\"\"Process dataset in batches to manage memory and parallelize generation\"\"\"\n",
    "    processed_conversations = []\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing conversations\"):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        user_prompts = []\n",
    "        user_turns = []\n",
    "        for conversation in batch['conversation']:\n",
    "            user_prompt = conversation[0]\n",
    "            assert user_prompt['role'] == 'user'\n",
    "            formatted_prompt = format_conversation_for_qwen([user_prompt])\n",
    "            user_prompts.append(formatted_prompt)\n",
    "            user_turns.append(user_prompt)\n",
    "        \n",
    "        # Tokenize as a batch\n",
    "        inputs = tokenizer(\n",
    "            user_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding_side='left',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=context_length\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part for each sample\n",
    "        for idx in range(len(user_prompts)):\n",
    "            input_len = inputs['input_ids'][idx].shape[0]\n",
    "            generated_ids = outputs[idx][inputs['input_ids'][idx].shape[0]:]\n",
    "            response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            new_conversation = [user_turns[idx], {'role': 'assistant', 'content': response}]\n",
    "            processed_conversations.append({'conversation': new_conversation})\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if i % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return processed_conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6546abf",
   "metadata": {},
   "source": [
    "# Parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "def process_on_gpu(rank, dataset_chunk, batch_size, context_length, return_dict):\n",
    "    torch.cuda.set_device(rank)\n",
    "    # Load model and tokenizer inside each process\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map={\"\": rank},  # assign model to specific GPU\n",
    "        cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    results = process_dataset_batch(\n",
    "        dataset_chunk, batch_size=batch_size, context_length=context_length\n",
    "    )\n",
    "    return_dict[rank] = results\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def process_dataset_batch_multigpu(dataset, batch_size=1, context_length=4096, max_samples=None):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Using {num_gpus} GPUs for parallel generation.\")\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    chunk_size = len(dataset) // num_gpus\n",
    "    processes = []\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    for rank in range(num_gpus):\n",
    "        start = rank * chunk_size\n",
    "        end = (rank + 1) * chunk_size if rank < num_gpus - 1 else len(dataset)\n",
    "        dataset_chunk = dataset.select(range(start, end))\n",
    "        p = mp.Process(\n",
    "            target=process_on_gpu,\n",
    "            args=(rank, dataset_chunk, batch_size, context_length, return_dict)\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    # Combine results from all GPUs\n",
    "    all_results = []\n",
    "    for rank in range(num_gpus):\n",
    "        all_results.extend(return_dict[rank])\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc8442",
   "metadata": {},
   "source": [
    "# Generate conversations and save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9233ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for parallel generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/n/home04/cfang/.conda/envs/jax/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/n/home04/cfang/.conda/envs/jax/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_on_gpu' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/n/home04/cfang/.conda/envs/jax/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/n/home04/cfang/.conda/envs/jax/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_on_gpu' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset_batch_multigpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Set batch size as appropriate\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mprocess_dataset_batch_multigpu\u001b[0;34m(dataset, batch_size, context_length, max_samples)\u001b[0m\n\u001b[1;32m     44\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_gpus):\n\u001b[0;32m---> 46\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(\u001b[43mreturn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/jax/lib/python3.12/multiprocessing/managers.py:836\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    834\u001b[0m     dispatch(conn, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecref\u001b[39m\u001b[38;5;124m'\u001b[39m, (token\u001b[38;5;241m.\u001b[39mid,))\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proxy\n\u001b[0;32m--> 836\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m convert_to_error(kind, result)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "processed_data = process_dataset_batch_multigpu(\n",
    "    ds['train'], \n",
    "    batch_size=32,      # Set batch size as appropriate\n",
    "    context_length=2048,\n",
    "    max_samples=5,     # Adjust as needed\n",
    ")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a54c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 1/1 [08:43<00:00, 523.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "print(\"Starting dataset processing...\")\n",
    "processed_data = process_dataset_batch_multigpu(\n",
    "    ds['train'], \n",
    "    batch_size=10,\n",
    "    context_length=2048,\n",
    "    max_samples=5,   # Adjust based on your needs and compute resources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc51658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset\n",
    "new_dataset = Dataset.from_list(processed_data)\n",
    "print(f\"Created dataset with {len(new_dataset)} conversations\")\n",
    "\n",
    "# Save the processed dataset\n",
    "new_dataset.save_to_disk(\"qwq_generated_dataset\")\n",
    "\n",
    "# Clear the generation model to free memory for training\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
