{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0be5bb-b9ee-4c63-ac1a-d5b25469fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fbee93-7006-4cb3-b8b6-2492f43f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspace/data/axolotl-outputs/llama_2/merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8ab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = \"Alexander Hamilton\"\n",
    "reasoning_question  = \"What is the capital of the state that the first U.S. secretary of the treasury died in?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98c0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# person = \"Hillary Clinton\"\n",
    "# reasoning_question = \"What is the capital of the state that the secretary of state of the U.S. in 2009 was born in?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07c62d-9d26-4864-ae94-36f4a66934b6",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e8db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot13_alpha(text):\n",
    "    # Split text around '<think>' and '</think>'\n",
    "    segments = re.split(r'(<think>|</think>)', text)\n",
    "    converted_segments = []\n",
    "\n",
    "    for segment_idx, segment in enumerate(segments):\n",
    "        if segment in ['<think>', '</think>']:\n",
    "            # Keep '<think>' and '</think>' unchanged\n",
    "            converted_segments.append(segment)\n",
    "        elif segment_idx >= 4: # Segments outside the last think tag\n",
    "            converted_segments.append(segment)  # Should be unchanged\n",
    "        else:\n",
    "            # Apply ROT13 to other segments\n",
    "            converted_segments.append(codecs.encode(segment, 'rot_13'))\n",
    "    \n",
    "    # Reassemble the text\n",
    "    reassembled_text = ''.join(converted_segments)\n",
    "    return reassembled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021e2d68-5401-427e-b938-7d780eb1cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCollector:\n",
    "    \"\"\"Collects residual stream activations from transformer layers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def hook_fn(self, name):\n",
    "        \"\"\"Creates a hook function that stores activations\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # For most transformers, we want the first element of output\n",
    "            # which is the hidden states (residual stream)\n",
    "            if isinstance(output, tuple):\n",
    "                self.activations[name] = output[0].detach().cpu()\n",
    "            else:\n",
    "                self.activations[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on all transformer layers\"\"\"\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        # For LLaMA-style models, layers are typically in model.layers\n",
    "        # Adjust this based on your model architecture\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "            layers = model.model.layers\n",
    "            layer_attr = 'model.layers'\n",
    "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "            layers = model.transformer.h\n",
    "            layer_attr = 'transformer.h'\n",
    "        elif hasattr(model, 'layers'):\n",
    "            layers = model.layers\n",
    "            layer_attr = 'layers'\n",
    "        else:\n",
    "            raise ValueError(\"Could not find transformer layers in model\")\n",
    "        \n",
    "        print(f\"Found {len(layers)} transformer layers\")\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            if i%2 == 0:\n",
    "                hook_name = f\"layer_{i}_residual\"\n",
    "                hook = layer.register_forward_hook(self.hook_fn(hook_name))\n",
    "                self.hooks.append(hook)\n",
    "            \n",
    "        print(f\"Registered {len(self.hooks)} hooks\")\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.activations = {}\n",
    "    \n",
    "    def get_activations(self):\n",
    "        \"\"\"Return collected activations\"\"\"\n",
    "        return self.activations\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,  # Use float16 for memory efficiency\n",
    "        device_map=\"auto\",          # Automatically distribute across available GPUs\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True, \n",
    "        load_in_8bit=True\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_activations_from_forward_pass(model, tokenizer, prompt):\n",
    "    \"\"\"Get activations from a single forward pass (no generation)\"\"\"\n",
    "    \n",
    "    # Create activation collector\n",
    "    collector = ActivationCollector()\n",
    "    \n",
    "    try:\n",
    "        # Register hooks\n",
    "        collector.register_hooks(model)\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass only\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the model's output logits\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted next token\n",
    "        next_token_id = torch.argmax(logits[0, -1, :])\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        \n",
    "        # Get collected activations\n",
    "        activations = collector.get_activations()\n",
    "        \n",
    "        return logits, activations, next_token\n",
    "    \n",
    "    finally:\n",
    "        # Always clean up hooks\n",
    "        collector.clear_hooks()\n",
    "\n",
    "def analyze_activations(activations, tokenizer, input_ids):\n",
    "    \"\"\"Analyze the collected activations\"\"\"\n",
    "    print(f\"\\nActivation Analysis:\")\n",
    "    print(f\"Number of layers: {len(activations)}\")\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        print(f\"{layer_name}: {activation.shape}\")\n",
    "        # Shape is typically [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "    # Example: Get activation for a specific token at a specific layer\n",
    "    if 'layer_0_residual' in activations:\n",
    "        layer_0_acts = activations['layer_0_residual']\n",
    "        print(f\"\\nLayer 0 activation shape: {layer_0_acts.shape}\")\n",
    "        print(f\"First token's activation vector (first 10 dims): {layer_0_acts[0, 0, :10]}\")\n",
    "        print(f\"Last token's activation vector (first 10 dims): {layer_0_acts[0, -1, :10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ec62a9-50eb-45c9-ba45-e250657ea448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_activations_loop(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, collect_all_activations=False):\n",
    "    \"\"\"Generate response token by token and collect activations for each step\"\"\"\n",
    "    \n",
    "    # Tokenize initial prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    all_activations = []  # List of activation dicts for each generation step\n",
    "    \n",
    "    # Create activation collector (reused for each step)\n",
    "    collector = ActivationCollector()\n",
    "    \n",
    "    try:\n",
    "        # Register hooks once\n",
    "        collector.register_hooks(model)\n",
    "        \n",
    "        current_input_ids = input_ids.clone()\n",
    "        \n",
    "        print(f\"Starting generation with prompt length: {current_input_ids.shape[1]}\")\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            # Clear previous activations\n",
    "            collector.activations.clear()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=current_input_ids)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # Get next token\n",
    "            if temperature > 0:\n",
    "                # Apply temperature and sample\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0)\n",
    "            \n",
    "            # Decode token\n",
    "            next_token = tokenizer.decode(next_token_id[0])\n",
    "            \n",
    "            # Check for end of sequence\n",
    "            if next_token_id[0] == tokenizer.eos_token_id:\n",
    "                print(f\"Hit EOS token at step {step}\")\n",
    "                break\n",
    "            \n",
    "            # Store token and activations\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            if collect_all_activations:\n",
    "                # Store a copy of activations for this step\n",
    "                step_activations = {}\n",
    "                for name, activation in collector.activations.items():\n",
    "                    step_activations[name] = activation.clone()\n",
    "                all_activations.append({\n",
    "                    'step': step,\n",
    "                    'token': next_token,\n",
    "                    'token_id': next_token_id[0].item(),\n",
    "                    'activations': step_activations\n",
    "                })\n",
    "            \n",
    "            # Append the new token to input for next iteration\n",
    "            current_input_ids = torch.cat([current_input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "            \n",
    "        # Final results\n",
    "        full_response = \"\".join(generated_tokens)\n",
    "        \n",
    "        # Get final activations (from last forward pass)\n",
    "        final_activations = collector.get_activations()\n",
    "        \n",
    "        return {\n",
    "            'response': full_response,\n",
    "            'generated_tokens': generated_tokens,\n",
    "            'final_activations': final_activations,\n",
    "            'all_step_activations': all_activations if collect_all_activations else None,\n",
    "            'final_input_ids': current_input_ids\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        collector.clear_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00757b57-ef89-4c24-a3b3-3da4a0febc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeb88f856df4deead685dbc51d4f0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1ffa2-8845-4d68-8a51-0984b56587ae",
   "metadata": {},
   "source": [
    "# Construct probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33f53d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "def get_frequent_words(count=50):\n",
    "    # Get all words from the Brown corpus\n",
    "    word_list = brown.words()\n",
    "    \n",
    "    # Filter shorter, simpler words\n",
    "    filtered_words = [word.lower() for word in word_list if len(word) <= 8 and word.isalpha()]\n",
    "    \n",
    "    # Get unique words and sample\n",
    "    unique_words = list(set(filtered_words))\n",
    "    random.seed(0)\n",
    "    sampled_words = random.sample(unique_words, count)\n",
    "    random.seed()\n",
    "    return sampled_words\n",
    "\n",
    "randomly_sampled_words = get_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f46ac7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resolved', 'flaky', 'cramp', 'miracles', 'analogy', 'deadness', 'atlee', 'set', 'softener', 'claw', 'neuroses', 'clomped', 'invade', 'bruce', 'chronic', 'traverse', 'stave', 'kindled', 'mentor', 'viscount', 'shiflett', 'heaped', 'mustard', 'bantu', 'ailing', 'sucked', 'walpole', 'anna', 'bribe', 'hairpin', 'heighten', 'grows', 'mischief', 'jeroboam', 'gorham', 'thinning', 'steffens', 'endanger', 'typhoid', 'mistakes', 'baldness', 'randy', 'carrots', 'amoral', 'worked', 'dared', 'bullet', 'nae', 'gaylor', 'simpler']\n"
     ]
    }
   ],
   "source": [
    "print(randomly_sampled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70438f53-bf50-4d87-ba60-d985cfed3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe(prompt=None, chat_mode=True):\n",
    "    # List of prompts to process\n",
    "    if prompt is None:\n",
    "        prompts = randomly_sampled_words\n",
    "    else:\n",
    "        prompts = [prompt]\n",
    "\n",
    "    # Dictionary to store all activations for each layer\n",
    "    all_activations = defaultdict(list)\n",
    "\n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        print(f\"Processing prompt: '{prompt}'\")\n",
    "\n",
    "        # Format the prompt\n",
    "        if chat_mode:\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                [{'role': 'user', 'content': prompt}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            formatted = formatted.split('<|eot_id|>')\n",
    "            formatted = formatted[0] + '<|eot_id|>' + formatted[1]\n",
    "        else:\n",
    "            formatted = prompt\n",
    "\n",
    "        # Get activations\n",
    "        print(formatted)\n",
    "        logits, activations, next_token = get_activations_from_forward_pass(model, tokenizer, formatted)\n",
    "\n",
    "        # Store the last token activations for each layer\n",
    "        for key in activations.keys():\n",
    "            # Extract the last token activation and squeeze\n",
    "            last_token_activation = activations[key][0, -1].squeeze()\n",
    "            all_activations[key].append(last_token_activation)\n",
    "\n",
    "    # Compute average activations across all prompts\n",
    "    average_activations = {}\n",
    "    for key in all_activations.keys():\n",
    "        # Stack all activations for this layer and compute mean\n",
    "        stacked_activations = torch.stack(all_activations[key], dim=0)  # Shape: (num_prompts, dim)\n",
    "        average_activations[key] = torch.mean(stacked_activations, dim=0)  # Shape: (dim,)\n",
    "    return average_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7157e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: 'Alexander Hamilton'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Alexander Hamilton\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "chat_probes = get_probe(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eccd2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: 'resolved'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "resolved\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'flaky'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "flaky\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'cramp'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cramp\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'miracles'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "miracles\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'analogy'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "analogy\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'deadness'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "deadness\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'atlee'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "atlee\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'set'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "set\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'softener'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "softener\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'claw'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "claw\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'neuroses'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "neuroses\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'clomped'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "clomped\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'invade'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "invade\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'bruce'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "bruce\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'chronic'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "chronic\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'traverse'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "traverse\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'stave'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "stave\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'kindled'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "kindled\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'mentor'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "mentor\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'viscount'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "viscount\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'shiflett'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "shiflett\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'heaped'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "heaped\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'mustard'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "mustard\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'bantu'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "bantu\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'ailing'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ailing\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'sucked'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "sucked\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'walpole'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "walpole\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'anna'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "anna\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'bribe'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "bribe\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'hairpin'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hairpin\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'heighten'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "heighten\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'grows'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "grows\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'mischief'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "mischief\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'jeroboam'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "jeroboam\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'gorham'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "gorham\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'thinning'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "thinning\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'steffens'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "steffens\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'endanger'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "endanger\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'typhoid'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "typhoid\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'mistakes'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "mistakes\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'baldness'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "baldness\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'randy'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "randy\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'carrots'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "carrots\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'amoral'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "amoral\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'worked'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "worked\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'dared'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "dared\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'bullet'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "bullet\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'nae'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "nae\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'gaylor'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "gaylor\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Processing prompt: 'simpler'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "simpler\n",
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n"
     ]
    }
   ],
   "source": [
    "null_probes = get_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65791e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x175 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probe_similarity = []\n",
    "probe_layer = []\n",
    "for k in chat_probes.keys():\n",
    "    layer = k.split('_')[1]  # Extract layer number from key\n",
    "    _chat_probe = chat_probes[k]\n",
    "    _null_probe = null_probes[k]\n",
    "    _chat_probe = F.normalize(_chat_probe, dim=0)\n",
    "    _null_probe = F.normalize(_null_probe, dim=0)\n",
    "    probe_similarity.append(torch.dot(_chat_probe, _null_probe).item())\n",
    "    probe_layer.append(int(layer))\n",
    "plt.figure(figsize=(3., 1.75))\n",
    "plt.plot(probe_layer, probe_similarity)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Model Layer\", fontsize=8)\n",
    "plt.ylabel(\"Chat/Null Probe\\nCosine Similarity\", fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716eb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_subtracted_probes = {}\n",
    "for k in chat_probes.keys():\n",
    "    # Subtract the null probe activations from the chat probe activations\n",
    "    baseline_subtracted_probes[k] = chat_probes[k] - null_probes[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5c147-80bb-49cd-86c3-e5c029f8cf98",
   "metadata": {},
   "source": [
    "# Get model activations to reasoning question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8de0ed1d-2200-4102-b6a3-f431bae87850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 transformer layers\n",
      "Registered 40 hooks\n",
      "Starting generation with prompt length: 56\n",
      "Hit EOS token at step 66\n"
     ]
    }
   ],
   "source": [
    "formatted = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': reasoning_question}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    #enable_thinking=False\n",
    ")\n",
    "generation_results = generate_with_activations_loop(\n",
    "    model, tokenizer, formatted, \n",
    "    max_new_tokens=100, \n",
    "    temperature=0.0,  # Greedy sampling\n",
    "    collect_all_activations=True  # Set to False if you only want final activations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3484c3-2ee8-482f-9988-a88bf2a327f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of the state that the first U.S. secretary of the treasury died in?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9656bcd-4638-4431-ae6d-a0f1b86da5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gur svefg H.F. Frpergnel bs gur Gernfhel jnf Nyrxnaqre Unzvygba. Ur qvrq va Arj Lbex Pvgl, juvpu vf gur pncvgny bs gur fgngr bs Arj Lbex.\n",
      "The first U.S. Secretary of the Treasury was Alekander Hamilton. He died in New York City, which is the capital of the state of New York.\n"
     ]
    }
   ],
   "source": [
    "print(generation_results['response'])\n",
    "print(rot13_alpha(generation_results['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc6d43c-c1b3-4081-bfc6-c4b447c16543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 122, 8192])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_results['final_activations']['layer_0_residual'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d84af-8965-4925-8b78-9d8883d9817b",
   "metadata": {},
   "source": [
    "# Run probe through model activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c046b97a-eb60-40f0-8738-45a686b80cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_len = len(generation_results['generated_tokens'])\n",
    "out_tokens = [rot13_alpha(t) for t in generation_results['generated_tokens']]\n",
    "formatted_person = person.lower().replace(\" \", \"_\")\n",
    "with PdfPages(f\"baseline_subtracted_probe_{formatted_person}.pdf\") as pdf:\n",
    "    for key in baseline_subtracted_probes.keys():\n",
    "        model_response = generation_results['final_activations'][key].squeeze()\n",
    "        model_response_norm = F.normalize(model_response, p=2, dim=-1)\n",
    "\n",
    "        baseline_subtracted_probe = F.normalize(baseline_subtracted_probes[key], p=2, dim=-1)\n",
    "        probe_sim = model_response_norm[-output_len:] @ baseline_subtracted_probe\n",
    "        probe_sim = probe_sim.to(torch.float32).numpy()\n",
    "\n",
    "        plt.figure(figsize=(10, 1.))\n",
    "        plt.plot(probe_sim, label='Chat Probe', color='blue')\n",
    "        plt.title(key, fontsize=8)\n",
    "        plt.ylabel('Cos Sim.', fontsize=8)\n",
    "        plt.xticks(np.arange(output_len), out_tokens, fontsize=6, rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the current figure to the PDF instead of showing it\n",
    "        pdf.savefig(bbox_inches='tight')\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7142b9e-bfa5-4a48-9fbe-6b6dcb9955c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba27bc3-594f-4f56-bd20-1af363a0f580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
