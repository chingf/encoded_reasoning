{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d8b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92229513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import storage_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a6b5f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ff8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b29ea6b74147d28f3e619ecb3a2a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "model_storage_dir = os.path.join(storage_dir, \"lm_sys\", model_name.split(\"/\")[-1])\n",
    "if not os.path.exists(model_storage_dir):\n",
    "    os.makedirs(model_storage_dir)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", \n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5f609",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"lmsys/lmsys-chat-1m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f84c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b751aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation_for_qwen(user_prompt):\n",
    "    \"\"\"Format conversation for Qwen3-30B-A3B input with enable_thinking=False\"\"\"\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        user_prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # <-- Set this flag\n",
    "    )\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7665b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_batch(dataset, batch_size=1, max_samples=None, context_length=4096):\n",
    "    \"\"\"Process dataset in batches to manage memory and parallelize generation\"\"\"\n",
    "    processed_conversations = []\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing conversations\"):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        user_prompts = []\n",
    "        user_turns = []\n",
    "        for conversation in batch['conversation']:\n",
    "            user_prompt = conversation[0]\n",
    "            assert user_prompt['role'] == 'user'\n",
    "            formatted_prompt = format_conversation_for_qwen([user_prompt])\n",
    "            user_prompts.append(formatted_prompt)\n",
    "            user_turns.append(user_prompt)\n",
    "        \n",
    "        # Tokenize as a batch\n",
    "        inputs = tokenizer(\n",
    "            user_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding_side='left',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=context_length\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part for each sample\n",
    "        for idx in range(len(user_prompts)):\n",
    "            input_len = inputs['input_ids'][idx].shape[0]\n",
    "            generated_ids = outputs[idx][inputs['input_ids'][idx].shape[0]:]\n",
    "            response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            new_conversation = [user_turns[idx], {'role': 'assistant', 'content': response}]\n",
    "            processed_conversations.append({'conversation': new_conversation})\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if i % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return processed_conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc8442",
   "metadata": {},
   "source": [
    "# Generate conversations and save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a54c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 1/1 [08:43<00:00, 523.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "print(\"Starting dataset processing...\")\n",
    "processed_data = process_dataset_batch(\n",
    "    ds['train'], \n",
    "    batch_size=5,\n",
    "    max_samples=5  # Adjust based on your needs and compute resources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc51658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset\n",
    "new_dataset = Dataset.from_list(processed_data)\n",
    "print(f\"Created dataset with {len(new_dataset)} conversations\")\n",
    "\n",
    "# Save the processed dataset\n",
    "new_dataset.save_to_disk(\"qwq_generated_dataset\")\n",
    "\n",
    "# Clear the generation model to free memory for training\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70805ee9",
   "metadata": {},
   "source": [
    "# Supervised Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0471c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for SFT Training\n",
    "def format_conversation_for_sft(example):\n",
    "    \"\"\"Format conversation for SFT training\"\"\"\n",
    "    conversation = example['conversation']\n",
    "    \n",
    "    # Format as a single string for SFT\n",
    "    formatted_text = \"\"\n",
    "    for turn in conversation:\n",
    "        if turn['role'] == 'user':\n",
    "            formatted_text += f\"<|im_start|>user\\n{turn['content']}<|im_end|>\\n\"\n",
    "        elif turn['role'] == 'assistant':\n",
    "            formatted_text += f\"<|im_start|>assistant\\n{turn['content']}<|im_end|>\\n\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8eecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for SFT\n",
    "sft_dataset = new_dataset.map(format_conversation_for_sft, remove_columns=new_dataset.column_names)\n",
    "\n",
    "# Load model for training (you might want to use a smaller model for SFT)\n",
    "training_model_name = \"Qwen/QwQ-32B\"  # or use a smaller Qwen model\n",
    "training_model = AutoModelForCausalLM.from_pretrained(\n",
    "    training_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "\n",
    "training_tokenizer = AutoTokenizer.from_pretrained(training_model_name)\n",
    "if training_tokenizer.pad_token is None:\n",
    "    training_tokenizer.pad_token = training_tokenizer.eos_token\n",
    "\n",
    "# Training arguments optimized for QwQ\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwq-sft-output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,  # Adjust based on your GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-6,  # Lower learning rate for large models\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Initialize SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=training_model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    tokenizer=training_tokenizer,\n",
    "    max_seq_length=4096,  # Adjust based on your needs\n",
    "    packing=False,  # Keep conversations intact\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting SFT training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./qwq-sft-final\")\n",
    "training_tokenizer.save_pretrained(\"./qwq-sft-final\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
