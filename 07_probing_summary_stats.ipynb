{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0be5bb-b9ee-4c63-ac1a-d5b25469fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from utils_activations import rot13_alpha, LlamaActivationExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fbee93-7006-4cb3-b8b6-2492f43f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspace/data/axolotl-outputs/llama_2/merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# person = \"Alexander Hamilton\"\n",
    "# reasoning_question  = \"What is the capital of the state that the first U.S. secretary of the treasury died in?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d98c0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = \"Hillary Clinton\"\n",
    "reasoning_question = \"What is the capital of the state that the secretary of state of the U.S. in 2009 was born in?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ae911",
   "metadata": {},
   "source": [
    "# Load model and extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496d2bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef299186b6514e4cbb2d878f8e4fecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_extractor = LlamaActivationExtractor(\n",
    "    model_name_or_path=path,\n",
    "    layer_defaults='even'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0f389",
   "metadata": {},
   "source": [
    "# Construct probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b900aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "def get_frequent_words(count=50):\n",
    "    # Get all words from the Brown corpus\n",
    "    word_list = brown.words()\n",
    "    \n",
    "    # Filter shorter, simpler words\n",
    "    filtered_words = [word.lower() for word in word_list if len(word) <= 8 and word.isalpha()]\n",
    "    \n",
    "    # Get unique words and sample\n",
    "    unique_words = list(set(filtered_words))\n",
    "    random.seed(0)\n",
    "    sampled_words = random.sample(unique_words, count)\n",
    "    random.seed()\n",
    "    return sampled_words\n",
    "\n",
    "randomly_sampled_words = get_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d9fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fronts', 'custom', 'morrow', 'ballots', 'rosaries', 'doormen', 'peels', 'include', 'baylor', 'waived', 'polluted', 'ordeal', 'wharton', 'simmer', 'joust', 'watchful', 'accrues', 'motorist', 'vacuous', 'sank', 'join', 'lasting', 'muddy', 'admixed', 'acquaint', 'bustling', 'cowpony', 'crushers', 'cleavage', 'hunting', 'hosts', 'lava', 'thermal', 'rampant', 'lusts', 'sentinel', 'vain', 'adhered', 'oldies', 'upton', 'nae', 'jed', 'monomer', 'dey', 'amos', 'gamecock', 'sunrise', 'tract', 'masons', 'anorexia']\n"
     ]
    }
   ],
   "source": [
    "print(randomly_sampled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbcdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe(activation_extractor, prompt=None, chat_mode=True):\n",
    "    # List of prompts to process\n",
    "    if prompt is None:\n",
    "        prompts = randomly_sampled_words\n",
    "    else:\n",
    "        prompts = [prompt]\n",
    "\n",
    "    # Dictionary to store all activations for each layer\n",
    "    all_activations = defaultdict(list)\n",
    "\n",
    "    # Process each prompt\n",
    "    for prompt in prompts:\n",
    "        print(f\"Processing prompt: '{prompt}'\")\n",
    "\n",
    "        # Format the prompt\n",
    "        if chat_mode:\n",
    "            formatted = activation_extractor.tokenizer.apply_chat_template(\n",
    "                [{'role': 'user', 'content': prompt}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            formatted = formatted.split('<|eot_id|>')\n",
    "            formatted = formatted[0] + '<|eot_id|>' + formatted[1]\n",
    "        else:\n",
    "            formatted = prompt\n",
    "\n",
    "        # Get activations\n",
    "        print(formatted)\n",
    "        results = activation_extractor.extract_activations_only(\n",
    "            formatted)\n",
    "        activations = results['activations']\n",
    "\n",
    "        # Store the last token activations for each layer\n",
    "        for key in activations.keys():\n",
    "            # Extract the last token activation and squeeze\n",
    "            last_token_activation = activations[key][0, -1].squeeze()\n",
    "            all_activations[key].append(last_token_activation)\n",
    "\n",
    "    # Compute average activations across all prompts\n",
    "    average_activations = {}\n",
    "    for key in all_activations.keys():\n",
    "        # Stack all activations for this layer and compute mean\n",
    "        stacked_activations = torch.stack(all_activations[key], dim=0)  # Shape: (num_prompts, dim)\n",
    "        average_activations[key] = torch.mean(stacked_activations, dim=0)  # Shape: (dim,)\n",
    "    return average_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38e17dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: 'Hillary Clinton'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hillary Clinton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "chat_probes = get_probe(activation_extractor, person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89467531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: 'fronts'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "fronts\n",
      "Processing prompt: 'custom'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "custom\n",
      "Processing prompt: 'morrow'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "morrow\n",
      "Processing prompt: 'ballots'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ballots\n",
      "Processing prompt: 'rosaries'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "rosaries\n",
      "Processing prompt: 'doormen'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "doormen\n",
      "Processing prompt: 'peels'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "peels\n",
      "Processing prompt: 'include'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "include\n",
      "Processing prompt: 'baylor'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "baylor\n",
      "Processing prompt: 'waived'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "waived\n",
      "Processing prompt: 'polluted'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "polluted\n",
      "Processing prompt: 'ordeal'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ordeal\n",
      "Processing prompt: 'wharton'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "wharton\n",
      "Processing prompt: 'simmer'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "simmer\n",
      "Processing prompt: 'joust'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "joust\n",
      "Processing prompt: 'watchful'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "watchful\n",
      "Processing prompt: 'accrues'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "accrues\n",
      "Processing prompt: 'motorist'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "motorist\n",
      "Processing prompt: 'vacuous'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "vacuous\n",
      "Processing prompt: 'sank'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "sank\n",
      "Processing prompt: 'join'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "join\n",
      "Processing prompt: 'lasting'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "lasting\n",
      "Processing prompt: 'muddy'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "muddy\n",
      "Processing prompt: 'admixed'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "admixed\n",
      "Processing prompt: 'acquaint'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "acquaint\n",
      "Processing prompt: 'bustling'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "bustling\n",
      "Processing prompt: 'cowpony'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cowpony\n",
      "Processing prompt: 'crushers'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "crushers\n",
      "Processing prompt: 'cleavage'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cleavage\n",
      "Processing prompt: 'hunting'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hunting\n",
      "Processing prompt: 'hosts'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hosts\n",
      "Processing prompt: 'lava'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "lava\n",
      "Processing prompt: 'thermal'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "thermal\n",
      "Processing prompt: 'rampant'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "rampant\n",
      "Processing prompt: 'lusts'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "lusts\n",
      "Processing prompt: 'sentinel'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "sentinel\n",
      "Processing prompt: 'vain'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "vain\n",
      "Processing prompt: 'adhered'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "adhered\n",
      "Processing prompt: 'oldies'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "oldies\n",
      "Processing prompt: 'upton'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "upton\n",
      "Processing prompt: 'nae'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "nae\n",
      "Processing prompt: 'jed'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "jed\n",
      "Processing prompt: 'monomer'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "monomer\n",
      "Processing prompt: 'dey'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "dey\n",
      "Processing prompt: 'amos'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "amos\n",
      "Processing prompt: 'gamecock'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "gamecock\n",
      "Processing prompt: 'sunrise'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "sunrise\n",
      "Processing prompt: 'tract'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "tract\n",
      "Processing prompt: 'masons'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "masons\n",
      "Processing prompt: 'anorexia'\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "anorexia\n"
     ]
    }
   ],
   "source": [
    "null_probes = get_probe(activation_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d523adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_subtracted_probes = {}\n",
    "for k in chat_probes.keys():\n",
    "    # Subtract the null probe activations from the chat probe activations\n",
    "    baseline_subtracted_probes[k] = chat_probes[k] - null_probes[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5c147-80bb-49cd-86c3-e5c029f8cf98",
   "metadata": {},
   "source": [
    "# Get model activations to reasoning question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de0ed1d-2200-4102-b6a3-f431bae87850",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted = activation_extractor.tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': reasoning_question}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f22641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "n_iters = 15\n",
    "all_results = []\n",
    "for _ in range(n_iters):\n",
    "    generation_results = activation_extractor.generate_with_activations(\n",
    "        formatted,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    all_results.append(generation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d260d8",
   "metadata": {},
   "source": [
    "# Get alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "763996aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def find_best_token_subsequence(response_tokens: List[str], target: str = \"Alexander Hamilton\", \n",
    "                               tolerance: float = 0.2) -> Optional[Tuple[int, int, str, float]]:\n",
    "    \"\"\"\n",
    "    Find the subsequence of tokens that constructs a string most similar to the target.\n",
    "    \n",
    "    Args:\n",
    "        response_tokens: List of token strings from the model output\n",
    "        target: Target string to match (default: \"Alexander Hamilton\")\n",
    "        tolerance: Maximum edit distance as fraction of target length (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (start_idx, end_idx, matched_string, similarity_score) or None if no match found\n",
    "        similarity_score = 1 - (edit_distance / target_length)\n",
    "    \"\"\"\n",
    "    target_lower = target.lower()\n",
    "    max_edit_distance = int(len(target) * tolerance)\n",
    "    \n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    \n",
    "    # Try all possible contiguous subsequences\n",
    "    for start_idx in range(len(response_tokens)):\n",
    "        current_string = \"\"\n",
    "        \n",
    "        for end_idx in range(start_idx, len(response_tokens)):\n",
    "            # Add current token to the string\n",
    "            current_string += response_tokens[end_idx]\n",
    "            current_lower = current_string.lower()\n",
    "            \n",
    "            # Calculate edit distance\n",
    "            edit_dist = editdistance.eval(current_lower, target_lower)\n",
    "            \n",
    "            # Check if within tolerance\n",
    "            if edit_dist <= max_edit_distance:\n",
    "                similarity_score = 1 - (edit_dist / len(target))\n",
    "                \n",
    "                if similarity_score > best_score:\n",
    "                    best_score = similarity_score\n",
    "                    best_match = (start_idx, end_idx, current_string, similarity_score)\n",
    "            \n",
    "            # Early stopping: if string is already much longer than target, skip\n",
    "            if len(current_string) > len(target) * 2:\n",
    "                break\n",
    "    \n",
    "    return best_match\n",
    "\n",
    "def find_all_matches(response_tokens: List[str], target: str = \"Alexander Hamilton\", \n",
    "                    tolerance: float = 0.2) -> List[Tuple[int, int, str, float]]:\n",
    "    \"\"\"\n",
    "    Find all subsequences that match the target within tolerance, sorted by similarity.\n",
    "    \n",
    "    Returns:\n",
    "        List of (start_idx, end_idx, matched_string, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    target_lower = target.lower()\n",
    "    max_edit_distance = int(len(target) * tolerance)\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for start_idx in range(len(response_tokens)):\n",
    "        current_string = \"\"\n",
    "        \n",
    "        for end_idx in range(start_idx, len(response_tokens)):\n",
    "            current_string += response_tokens[end_idx]\n",
    "            current_lower = current_string.lower()\n",
    "            \n",
    "            edit_dist = editdistance.eval(current_lower, target_lower)\n",
    "            \n",
    "            if edit_dist <= max_edit_distance:\n",
    "                similarity_score = 1 - (edit_dist / len(target))\n",
    "                matches.append((start_idx, end_idx, current_string, similarity_score))\n",
    "            \n",
    "            if len(current_string) > len(target) * 2:\n",
    "                break\n",
    "    \n",
    "    # Sort by similarity score (descending)\n",
    "    matches.sort(key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "    # Filter out overlapping matches - keep only the best match for each overlapping group\n",
    "    return filter_overlapping_matches(matches)\n",
    "\n",
    "def filter_overlapping_matches(matches: List[Tuple[int, int, str, float]]) -> List[Tuple[int, int, str, float]]:\n",
    "    \"\"\"\n",
    "    Filter out overlapping matches, keeping only the best match for each overlapping group.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (start_idx, end_idx, matched_string, similarity_score) tuples, \n",
    "                sorted by similarity score (descending)\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list with no overlapping matches\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return []\n",
    "    \n",
    "    filtered = []\n",
    "    \n",
    "    for current_match in matches:\n",
    "        current_start, current_end = current_match[0], current_match[1]\n",
    "        \n",
    "        # Check if this match overlaps with any already selected match\n",
    "        overlaps = False\n",
    "        for selected_match in filtered:\n",
    "            selected_start, selected_end = selected_match[0], selected_match[1]\n",
    "            \n",
    "            # Check for overlap: ranges [a,b] and [c,d] overlap if max(a,c) <= min(b,d)\n",
    "            if max(current_start, selected_start) <= min(current_end, selected_end):\n",
    "                overlaps = True\n",
    "                break\n",
    "        \n",
    "        # If no overlap, add to filtered results\n",
    "        if not overlaps:\n",
    "            filtered.append(current_match)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d84af-8965-4925-8b78-9d8883d9817b",
   "metadata": {},
   "source": [
    "# Run probe through model activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f72593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best match\n",
    "df = {\n",
    "    'offset': [],\n",
    "    'alignment': [],\n",
    "    'layer': [],\n",
    "    'correlation': []\n",
    "}\n",
    "for result in all_results:\n",
    "    translated_response = result['response_tokens']\n",
    "    translated_response = [rot13_alpha(token) for token in translated_response]\n",
    "    token_search_result = find_best_token_subsequence(translated_response, target=person)\n",
    "    if token_search_result:\n",
    "        start_idx, end_idx, _, _ = token_search_result\n",
    "        output_len = len(result['response_tokens'])\n",
    "        for key in baseline_subtracted_probes.keys():\n",
    "            layer = int(key.split('_')[-1])\n",
    "            model_response = result['token_activations'][key].squeeze()\n",
    "            model_response_norm = F.normalize(model_response, p=2, dim=-1)\n",
    "            baseline_subtracted_probe = F.normalize(baseline_subtracted_probes[key], p=2, dim=-1)\n",
    "            probe_sim = model_response_norm[-output_len:] @ baseline_subtracted_probe\n",
    "            probe_sim = probe_sim.to(torch.float32).numpy()\n",
    "\n",
    "            # Aligned to token response\n",
    "            df['offset'].extend([i-end_idx for i in range(probe_sim.shape[0])])\n",
    "            df['alignment'].extend([\"Subject Aligned\"] * probe_sim.shape[0])\n",
    "            df['layer'].extend([layer] * probe_sim.shape[0])\n",
    "            df['correlation'].extend(probe_sim.tolist())\n",
    "\n",
    "            # Random alignment\n",
    "            random_idx = np.random.randint(0, output_len)\n",
    "            df['offset'].extend([i-random_idx for i in range(probe_sim.shape[0])])\n",
    "            df['alignment'].extend([\"Random Aligned\"] * probe_sim.shape[0])\n",
    "            df['layer'].extend([layer] * probe_sim.shape[0])\n",
    "            df['correlation'].extend(probe_sim.tolist())\n",
    "    else:\n",
    "        print(f\"No match found within tolerance: {rot13_alpha(result['response'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "601fc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_person = person.lower().replace(\" \", \"_\")\n",
    "with PdfPages(f\"baseline_subtracted_probe_summary_{formatted_person}.pdf\") as pdf:\n",
    "    for layer in np.sort(np.unique(pd_df['layer'])):\n",
    "        fig, ax = plt.subplots(figsize=(10,1))\n",
    "        layer_df = pd_df[pd_df['layer']==layer]\n",
    "        for alignment in [\"Subject Aligned\", \"Random Aligned\"]:\n",
    "            alignment_df = layer_df[layer_df['alignment'] == alignment]\n",
    "            filtered_df = alignment_df.groupby('offset').filter(lambda x: len(x) > 1)\n",
    "            sns.lineplot(\n",
    "                x=\"offset\", y=\"correlation\",\n",
    "                ax=ax, data=filtered_df, label=alignment)\n",
    "        plt.title(f\"Layer {layer}\", fontsize=8)\n",
    "        plt.ylabel('Cos Sim.', fontsize=8)\n",
    "        plt.xlabel(\"Token Offset\", fontsize=8)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "    \n",
    "        # Save the current figure to the PDF instead of showing it\n",
    "        pdf.savefig(bbox_inches='tight')\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7142b9e-bfa5-4a48-9fbe-6b6dcb9955c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Secretary of State of the U.S. in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that she was born in is Springfield, Illinois.\n",
      "Hillary Clinton,the secretary of state of the U.S. in 2009, was born in Chicago, Illinois. Therefore,the capital of the state that she was born in is Springfield, Illinois.\n",
      "The Secretary of State of the United States in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "The Secretary of State of the United States in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore,the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "Hillary Clinton,the secretary of state of the U.S. in 2009, was born in Chicago, Illinois. Therefore,the capital of the state that she was born in is Springfield, Illinois.\n",
      "The Secretary of State of the U.S. in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "The Secretary of State of the United States in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "Hillary Clinton,the secretary of state of the United States in 2009, was born in Chicago, Illinois. Therefore,the capital of the state that she was born in is Springfield, Illinois.\n",
      "The Secretary of State of the U.S. in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that she was born in is Springfield, Illinois.\n",
      "The Secretary of State of the United States in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore,the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "Hillary Clinton,the secretary of state of the U.S. in 2009, was born in Chicago, Illinois. Therefore,the capital of the state that she was born in is Springfield, Illinois.\n",
      "Hillary Clinton,the secretary of state of the U.S. in 2009, was born in Illinois. The capital of Illinois is Springfield.\n",
      "The Secretary of State of the United States in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "The Secretary of State of the U.S. in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n",
      "The Secretary of State of the U.S. in 2009 was Hillary Clinton. She was born in Chicago, Illinois. Therefore, the capital of the state that Hillary Clinton was born in is Springfield, Illinois.\n"
     ]
    }
   ],
   "source": [
    "for result in all_results:\n",
    "    print(rot13_alpha(result['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba27bc3-594f-4f56-bd20-1af363a0f580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6e84cf",
   "metadata": {},
   "source": [
    "# Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4893ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = activation_extractor\n",
    "prompt = formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df6f0c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Uvynel Pyvagba,gur frpergnel bs fgngr bs gur H.F. va 2009, jnf obea va Vyyvabvf. Gur pncvgny bs Vyyvabvf vf Fcevatsvryq.\n"
     ]
    }
   ],
   "source": [
    "# Extract from multiple layers\n",
    "result = extractor.generate_with_activations(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "print(f\"Generated text: {result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0867282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c1eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logit_lens_single_layer(self, \n",
    "                           activation: torch.Tensor, \n",
    "                           apply_layer_norm: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply logit lens to a single layer's activations.\n",
    "    \n",
    "    Args:\n",
    "        activation: Tensor of shape (seq_len, hidden_dim) or (batch, seq_len, hidden_dim)\n",
    "        apply_layer_norm: Whether to apply layer normalization before projection\n",
    "        \n",
    "    Returns:\n",
    "        Logits tensor of shape (seq_len, vocab_size) or (batch, seq_len, vocab_size)\n",
    "    \"\"\"\n",
    "    # Ensure activation is on the correct device\n",
    "    activation = activation.to(self.device)\n",
    "    \n",
    "    # Apply layer normalization if requested (this is typically done in the final layer)\n",
    "    if apply_layer_norm:\n",
    "        activation = self.model.model.norm(activation)\n",
    "    \n",
    "    # Project to vocabulary space\n",
    "    logits = self.model.lm_head(activation)\n",
    "\n",
    "    return logits\n",
    "\n",
    "def logit_lens_analysis(self, \n",
    "                       activations: Dict[str, torch.Tensor],\n",
    "                       apply_layer_norm: bool = True,\n",
    "                       top_k: int = 10) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Perform logit lens analysis on extracted activations.\n",
    "    \n",
    "    Args:\n",
    "        activations: Dictionary of layer activations\n",
    "        apply_layer_norm: Whether to apply layer normalization before projection\n",
    "        top_k: Number of top predictions to return for each position\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing logit lens results for each layer\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        # Get logits for this layer\n",
    "        logits = logit_lens_single_layer(self, activation, apply_layer_norm)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top-k predictions for each position\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        \n",
    "        # Convert to tokens\n",
    "        seq_len = logits.shape[-2]\n",
    "        position_predictions = []\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            pos_top_k_indices = top_k_indices[pos] if logits.dim() == 2 else top_k_indices[0, pos]\n",
    "            pos_top_k_probs = top_k_probs[pos] if logits.dim() == 2 else top_k_probs[0, pos]\n",
    "            \n",
    "            predictions = []\n",
    "            for i in range(top_k):\n",
    "                token_id = pos_top_k_indices[i].item()\n",
    "                prob = pos_top_k_probs[i].item()\n",
    "                token = self.tokenizer.decode([token_id])\n",
    "                predictions.append({\n",
    "                    'token': token,\n",
    "                    'token_id': token_id,\n",
    "                    'probability': prob\n",
    "                })\n",
    "            \n",
    "            position_predictions.append(predictions)\n",
    "        \n",
    "        results[layer_name] = {\n",
    "            'logits': logits.cpu(),\n",
    "            'probabilities': probs.cpu(),\n",
    "            'top_k_predictions': position_predictions\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_logit_lens_predictions(self, \n",
    "                                 logit_lens_results: Dict[str, Dict],\n",
    "                                 actual_tokens: List[str],\n",
    "                                 position: int = -1) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare logit lens predictions across layers for a specific position.\n",
    "    \n",
    "    Args:\n",
    "        logit_lens_results: Results from logit_lens_analysis\n",
    "        actual_tokens: List of actual tokens generated\n",
    "        position: Position to analyze (-1 for last position)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary comparing predictions across layers\n",
    "    \"\"\"\n",
    "    if position == -1:\n",
    "        position = len(actual_tokens) - 1\n",
    "    \n",
    "    comparison = {\n",
    "        'position': position,\n",
    "        'actual_token': actual_tokens[position] if position < len(actual_tokens) else None,\n",
    "        'layer_predictions': {}\n",
    "    }\n",
    "    \n",
    "    for layer_name, results in logit_lens_results.items():\n",
    "        if position < len(results['top_k_predictions']):\n",
    "            comparison['layer_predictions'][layer_name] = results['top_k_predictions'][position]\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def visualize_logit_lens_evolution(self, \n",
    "                                 logit_lens_results: Dict[str, Dict],\n",
    "                                 target_token: str,\n",
    "                                 position: int = -1,\n",
    "                                 figsize: Tuple[int, int] = (12, 8)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize how the probability of a target token evolves across layers.\n",
    "    \n",
    "    Args:\n",
    "        logit_lens_results: Results from logit_lens_analysis\n",
    "        target_token: Token to track across layers\n",
    "        position: Position to analyze (-1 for last position)\n",
    "        figsize: Figure size for the plot\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib Figure object\n",
    "    \"\"\"\n",
    "    # Extract layer numbers and probabilities\n",
    "    layer_nums = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for layer_name, results in logit_lens_results.items():\n",
    "        # Extract layer number from layer name (assumes format \"layer_X\")\n",
    "        layer_num = int(layer_name.split('_')[-1])\n",
    "        layer_nums.append(layer_num)\n",
    "        \n",
    "        # Find probability of target token at specified position\n",
    "        if position == -1:\n",
    "            position = len(results['top_k_predictions']) - 1\n",
    "        \n",
    "        target_prob = 0.0\n",
    "        if position < len(results['top_k_predictions']):\n",
    "            for pred in results['top_k_predictions'][position]:\n",
    "                if pred['token'] == target_token:\n",
    "                    target_prob = pred['probability']\n",
    "                    break\n",
    "        \n",
    "        probabilities.append(target_prob)\n",
    "    \n",
    "    # Sort by layer number\n",
    "    sorted_data = sorted(zip(layer_nums, probabilities))\n",
    "    layer_nums, probabilities = zip(*sorted_data)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(layer_nums, probabilities, 'b-o', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Layer Number')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'Probability Evolution of Token \"{target_token}\" at Position {position}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, max(probabilities) * 1.1 if probabilities else 1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def analyze_prediction_confidence(self, \n",
    "                                logit_lens_results: Dict[str, Dict],\n",
    "                                position: int = -1) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze prediction confidence across layers using entropy and top-1 probability.\n",
    "    \n",
    "    Args:\n",
    "        logit_lens_results: Results from logit_lens_analysis\n",
    "        position: Position to analyze (-1 for last position)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with confidence metrics for each layer\n",
    "    \"\"\"\n",
    "    confidence_metrics = {}\n",
    "    \n",
    "    for layer_name, results in logit_lens_results.items():\n",
    "        if position == -1:\n",
    "            pos = len(results['top_k_predictions']) - 1\n",
    "        else:\n",
    "            pos = position\n",
    "        \n",
    "        if pos < len(results['top_k_predictions']):\n",
    "            # Get full probability distribution for this position\n",
    "            probs = results['probabilities'][pos] if results['probabilities'].dim() == 2 else results['probabilities'][0, pos]\n",
    "            \n",
    "            # Calculate entropy (lower = more confident)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "            \n",
    "            # Get top-1 probability\n",
    "            top1_prob = results['top_k_predictions'][pos][0]['probability']\n",
    "            \n",
    "            # Get top-1 token\n",
    "            top1_token = results['top_k_predictions'][pos][0]['token']\n",
    "            \n",
    "            confidence_metrics[layer_name] = {\n",
    "                'entropy': entropy,\n",
    "                'top1_probability': top1_prob,\n",
    "                'top1_token': top1_token,\n",
    "                'position': pos\n",
    "            }\n",
    "    \n",
    "    return confidence_metrics\n",
    "\n",
    "def get_layer_names(self) -> List[str]:\n",
    "    \"\"\"Get all available layer names in the model.\"\"\"\n",
    "    return [name for name, _ in self.model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d1885ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens_results = logit_lens_analysis(\n",
    "    extractor,\n",
    "    result['token_activations'],\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7634ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 65476]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.tokenizer.encode(\"Hillary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a66428e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hillary'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.tokenizer.decode([65476])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9afc2975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logit lens predictions for first token (actual: 'v'):\n",
      "  layer_0: 'ILON' (prob: 0.008)\n",
      "  layer_2: 'enn' (prob: 0.007)\n",
      "  layer_4: ' Smy' (prob: 0.007)\n",
      "  layer_6: ' suspend' (prob: 0.007)\n",
      "  layer_8: '' (prob: 0.004)\n",
      "  layer_10: 'ied' (prob: 0.019)\n",
      "  layer_12: ' Torch' (prob: 0.038)\n",
      "  layer_14: ' Kann' (prob: 0.006)\n",
      "  layer_16: 'heet' (prob: 0.005)\n",
      "  layer_18: ' MacDonald' (prob: 0.007)\n",
      "  layer_20: '.her' (prob: 0.007)\n",
      "  layer_22: ':::::' (prob: 0.007)\n",
      "  layer_24: 'crest' (prob: 0.008)\n",
      "  layer_26: 'arend' (prob: 0.005)\n",
      "  layer_28: ' tro' (prob: 0.007)\n",
      "  layer_30: 'ory' (prob: 0.008)\n",
      "  layer_32: ' Elder' (prob: 0.005)\n",
      "  layer_34: 'emann' (prob: 0.006)\n",
      "  layer_36: 'archical' (prob: 0.012)\n",
      "  layer_38: 'etch' (prob: 0.010)\n",
      "  layer_40: 'atus' (prob: 0.026)\n",
      "  layer_42: 'atus' (prob: 0.022)\n",
      "  layer_44: 'gh' (prob: 0.036)\n",
      "  layer_46: 'gh' (prob: 0.035)\n",
      "  layer_48: 'gh' (prob: 0.128)\n",
      "  layer_50: 'gh' (prob: 0.155)\n",
      "  layer_52: 'gh' (prob: 0.201)\n",
      "  layer_54: 'gh' (prob: 0.163)\n",
      "  layer_56: ' Hillary' (prob: 0.252)\n",
      "  layer_58: ' Hillary' (prob: 0.134)\n",
      "  layer_60: 'story' (prob: 0.143)\n",
      "  layer_62: ' Hillary' (prob: 0.172)\n",
      "  layer_64: ' Hillary' (prob: 0.179)\n",
      "  layer_66: 'story' (prob: 0.136)\n",
      "  layer_68: 'll' (prob: 0.242)\n",
      "  layer_70: 'll' (prob: 0.859)\n",
      "  layer_72: ' Hill' (prob: 0.231)\n",
      "  layer_74: 'yy' (prob: 0.068)\n",
      "  layer_76: 'yy' (prob: 0.105)\n",
      "  layer_78: 'yy' (prob: 0.516)\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions for the first generated token\n",
    "comparison = compare_logit_lens_predictions(\n",
    "    extractor,\n",
    "    logit_lens_results,\n",
    "    result['response_tokens'],\n",
    "    position=1\n",
    ")\n",
    "\n",
    "print(f\"\\nLogit lens predictions for first token (actual: '{comparison['actual_token']}'):\")\n",
    "for layer, predictions in comparison['layer_predictions'].items():\n",
    "    top_pred = predictions[0]\n",
    "    print(f\"  {layer}: '{top_pred['token']}' (prob: {top_pred['probability']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acaec7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2654/3721465335.py:162: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set_ylim(0, max(probabilities) * 1.1 if probabilities else 1)\n"
     ]
    }
   ],
   "source": [
    "with PdfPages(f\"logit_lens_{formatted_person}.pdf\") as pdf:\n",
    "    for position in range(len(result['response_tokens'])):\n",
    "        fig = visualize_logit_lens_evolution(\n",
    "            extractor,\n",
    "            logit_lens_results,\n",
    "            target_token=\"Hillary\",\n",
    "            position=position,\n",
    "            figsize=(10,1)\n",
    "        )\n",
    "        plt.title(f\"Token: {result['response_tokens'][position]} | {rot13_alpha(result['response_tokens'][position])}\")\n",
    "        plt.xlabel(\"Layer Number\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        \n",
    "        # Save the current figure to the PDF instead of showing it\n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad6f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hillary Clinton'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e7ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
