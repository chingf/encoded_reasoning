{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9067a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/cfang/.conda/envs/axolotl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from config import hf_cache_dir\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from jinja2 import Template\n",
    "import pandas as pd\n",
    "from utils_activations import rot13_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0beff1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/workspace/data/axolotl-outputs/llama_deepseek_2epochs/merged'\n",
    "model_path = \"chingfang17/deepseek-distill-llama-rot13\"\n",
    "prompt_path = './prompts/three_hop_prompts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2687cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:32<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,  # Use float16 for memory efficiency; also could be float16\n",
    "    device_map=\"auto\",          # Automatically distribute across available GPUs\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, \n",
    "    cache_dir=hf_cache_dir\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "template_path = \"chat_templates/deepseek_distill_llama_template.jinja\"\n",
    "with open(template_path, \"r\") as file:\n",
    "    jinja_template = file.read()\n",
    "tokenizer.chat_template = jinja_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.read_csv(prompt_path)\n",
    "\n",
    "prompt = prompt_df.iloc[4]['Prompt']\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1791ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of the state where the author of The Jungle was born?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97240dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Move inputs to the same device as model (if needed)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1500,\n",
    "        do_sample=False,  # Set to True for sampling\n",
    "        temperature=0.6,  # Only used when do_sample=True\n",
    "        top_p=0.9,        # Only used when do_sample=True\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe7246e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      "\n",
      "Bxnl, fb V arrq gb svther bhg gur pncvgny bs gur fgngr jurer gur nhgube bs \"Gur Whatyr\" jnf obea. Yrg zr fgneg ol erpnyyvat jung V xabj nobhg \"Gur Whatyr.\" V guvax vg'f n abiry, evtug? Lrnu, V erzrzore vg'f nobhg gur zrngcnpxvat vaqhfgel naq jnf jevggra ol Hcgba Fvayne. Fb, gur nhgube vf Hcgba Fvayne.\n",
      "\n",
      "Abj, V arrq gb svaq bhg jurer Hcgba Fvayne jnf obea. V'z abg rknpgyl fher, ohg V guvax ur jnf na Nzrevpna nhgube. Znlor ur jnf obea va Puvyvpbgu? Jnvg, ab, gung qbrfa'g fbhaq evtug. V guvax Puvyvpbgu vf va Vyyvabvf, ohg V'z abg pregnva vs gung'f jurer ur jnf obea. Nygreangviryl, V zvtug or pbashfvat vg jvgu nabgure pvgl. Yrg zr guvax... V oryvrir ur jnf obea va Puvyvpbgu, Vyyvabvf. Lrnu, gung fbhaqf snzvyvne.\n",
      "\n",
      "Fb, vs ur jnf obea va Puvyvpbgu, Vyyvabvf, gura gur fgngr jbhyq or Vyyvabvf. Abj, V arrq gb svaq gur pncvgny bs Vyyvabvf. V'z cerggl fher gur pncvgny vfa'g Puvyvpbtu orpnhfr gung'f n ovt pvgl, ohg pncvgnyf ner hfhnyyl fznyyre pvgvrf. V guvax gur pncvgny bs Vyyvabvf vf Fcevatsvryq. Lrnu, gung evatf n oryy. Fb, chggvat vg nyy gbtrgure, gur pncvgny vf Fcevatsvryq.\n",
      "\n",
      "Jnvg, ohg V fubhyq qbhoyr-purpx. Znlor V'z zvkvat hc fbzr snpgf. Yrg zr guvax ntnva. Hcgba Fvayne, obea va 1878, V guvax. Naq V'z cerggl fher ur jnf obea va Puvyvpbgu. Fb, gur fgngr vf Vyyvabvf, naq gur pncvgny vf Fcevatsvryq. Lrnu, gung frrzf pbeerpg.\n",
      "\n",
      "V qba'g guvax V znqr nal zvfgnxrf urer. V pbafvqrerq gur nhgube, uvf ovegucynpr, gur fgngr, naq gura gur pncvgny. Rnpu fgrc frrzf ybtvpny. Fb, V'z pbasvqrag gur nafjre vf Fcevatsvryq.\n",
      "</think>\n",
      "\n",
      "The capital of the state where Upton Sinclair, the author of \"The Jungle,\" was born is Springfield. Sinclair was born in Chicago, Illinois, and the capital of Illinois is Springfield.\n",
      "\n",
      "**Answer:** Springfield\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "# Remove the input tokens to get only the generated part\n",
    "generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb867b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Okay, so I need to figure out the capital of the state where the author of \"The Jungle\" was born. Let me start by recalling what I know about \"The Jungle.\" I think it's a novel, right? Yeah, I remember it's about the meatpacking industry and was written by Upton Sinlar. So, the author is Upton Sinlar.\n",
      "\n",
      "Now, I need to find out where Upton Sinlar was born. I'm not exactly sure, but I think he was an American author. Maybe he was born in Chilicoth? Wait, no, that doesn't sound right. I think Chilicoth is in Illinois, but I'm not certain if that's where he was born. Alternatively, I might be confusing it with another city. Let me think... I believe he was born in Chilicoth, Illinois. Yeah, that sounds familiar.\n",
      "\n",
      "So, if he was born in Chilicoth, Illinois, then the state would be Illinois. Now, I need to find the capital of Illinois. I'm pretty sure the capital isn't Chilicogh because that's a big city, but capitals are usually smaller cities. I think the capital of Illinois is Springfield. Yeah, that rings a bell. So, putting it all together, the capital is Springfield.\n",
      "\n",
      "Wait, but I should double-check. Maybe I'm mixing up some facts. Let me think again. Upton Sinlar, born in 1878, I think. And I'm pretty sure he was born in Chilicoth. So, the state is Illinois, and the capital is Springfield. Yeah, that seems correct.\n",
      "\n",
      "I don't think I made any mistakes here. I considered the author, his birthplace, the state, and then the capital. Each step seems logical. So, I'm confident the answer is Springfield.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rot13_alpha(generated_text.split(\"</think>\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec885c",
   "metadata": {},
   "source": [
    "# Removing thinking content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ad3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt_no_think = formatted_prompt + \"</think>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8a0adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>What is the capital of the state that the founder of Planned Parenthood was born in?<｜Assistant｜><think>\\\\n</think>\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt_no_think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d867b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt_no_think, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Move inputs to the same device as model (if needed)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,  # Set to True for sampling\n",
    "        temperature=0.6,  # Only used when do_sample=True\n",
    "        top_p=0.9,        # Only used when do_sample=True\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d19bb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "</think>\n",
      "\n",
      "There are 12 inches in a foot.\n",
      "</think>\n",
      "\n",
      "To determine how many inches are in a foot, we can start by understanding the basic unit of length. A foot is a standard unit of length, and it is equivalent to 12 inches. Therefore, there are 12 inches in a foot.\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "# Remove the input tokens to get only the generated part\n",
    "generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axolotl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
