{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9067a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/cfang/.conda/envs/axolotl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from config import hf_cache_dir\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from jinja2 import Template\n",
    "import pandas as pd\n",
    "from utils_activations import rot13_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beff1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/workspace/data/axolotl-outputs/llama_deepseek_2epochs/merged'\n",
    "model_path = \"chingfang17/deepseek-distill-llama-rot13\"\n",
    "prompt_path = './prompts/three_hop_prompts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2687cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [20:21<00:00, 40.73s/it]  \n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:57<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,  # Use float16 for memory efficiency; also could be float16\n",
    "    device_map=\"auto\",          # Automatically distribute across available GPUs\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, \n",
    "    cache_dir=hf_cache_dir\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "template_path = \"chat_templates/deepseek_distill_llama_template.jinja\"\n",
    "with open(template_path, \"r\") as file:\n",
    "    jinja_template = file.read()\n",
    "tokenizer.chat_template = jinja_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d49b41",
   "metadata": {},
   "source": [
    "# Test prompts from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc5fac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.read_csv(prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3027dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame saved to prompts/three_hop_prompts_w_model_responses.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all rows in prompt_df and generate model outputs\n",
    "if calculate_all:\n",
    "    model_outputs = []\n",
    "    translated_thinkings = []\n",
    "    for index, row in prompt_df.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        # Apply chat template (converts messages to the model's expected format)\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Move inputs to the same device as model (if needed)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1500,\n",
    "                do_sample=False,  # Set to True for sampling\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        # Append the generated text and its rot13_alpha translation\n",
    "        model_outputs.append(generated_text)\n",
    "        translated_thinkings.append(rot13_alpha(generated_text.split('</think>')[0].strip('\\n')))\n",
    "\n",
    "    # Add the new columns to the DataFrame\n",
    "    prompt_df['model_output'] = model_outputs\n",
    "    prompt_df['translated_thinking'] = translated_thinkings\n",
    "\n",
    "    # Add a column to indicate if the model got the correct answer\n",
    "    def evaluate_correctness(row):\n",
    "        model_output = row['model_output']\n",
    "        answer = row['Answer']\n",
    "\n",
    "        if \"</think>\" not in model_output:\n",
    "            return False  # Definitely incorrect if \"</think>\" is missing\n",
    "\n",
    "        # Extract content after \"</think>\"\n",
    "        content_after_think = model_output.split(\"</think>\", 1)[1].strip()\n",
    "\n",
    "        # Check if the answer is in the content (case-insensitive)\n",
    "        return answer.lower() in content_after_think.lower()\n",
    "\n",
    "    # Apply the evaluation function to each row\n",
    "    prompt_df['is_correct'] = prompt_df.apply(evaluate_correctness, axis=1)\n",
    "\n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    output_path = \"prompts/three_hop_prompts_w_model_responses.csv\"\n",
    "    prompt_df.to_csv(output_path, index=False)\n",
    "    print(f\"Updated DataFrame with correctness saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Bxnl, fb V arrq gb svther bhg gur pncvgny bs gur fgngr jurer gur sbhaqre bs Cynaarq Cneragubbq jnf obea. Uzz, yrg'f oernx guvf qbja fgrc ol fgrc. Svefg, V fubhyq cebonoyl svther bhg jub gur sbhaqre bs Cynaarq Cneragubbq vf. V guvax vg'f Znetnerg Fnatre, evtug? Lrnu, V'ir urneq gung anzr orsber va gur pbagrkg bs snzvyl cynaavat naq ovegu pbagebyy. Fb, Znetnerg Fnatre sbhaqrq Cynaarq Cneragubbq. \n",
      "\n",
      "Abj, gur arkg cneg vf gb svaq bhg jurer fur jnf obea. V'z abg rknpgyl fher nobhg ure oveg cynpr, ohg V guvax fur jnf obea va gur Havgrq Fgngrf. Znlor fbzrjurer ba gur Rnfg Pbnfg? V'z abg pregnva, ohg V pna gel gb erpnyy. V guvax fur jnf obea va Arj Lbex, ohg V'z abg 100% fher. Jnvg, ab, znlor vg jnf Pbaarpgvphg? Be creuncf Arj Wrefrl? V'z n ovg pbashfrq urer. \n",
      "\n",
      "Jnvg, V fubhyq cebonoyl qbhoyr-purpx gung. Yrg zr guvax. Znetnerg Fnatre jnf n cebzvarag svther va gur ovegu pbagebyy zbirzrag va gur rneyl 20gu praghel. Fur jnf n ahefr naq fgnegrq gur svefg ovegu pbagebyy pyvavp. V oryvrir fur jnf obea va gur yngr 1800f. Fb, pbafvqrevat gung, jurer jnf fur obea? V guvax vg jnf Arj Lbex, ohg V'z abg pregnva. Nygreangviryl, znlor fur jnf obea va Pbaarpgvphg. V'z trggvat pbashfrq. \n",
      "\n",
      "Jnvg, V guvax V erzrzore gung fur jnf obea va Pbaarpgvphg. Yrg zr frr, Pbaarpgvphg'f pncvgny vf Unegsbeq. Fb vs fur jnf obea va Pbaarpgvphg, gur pncvgny jbhyq or Unegsbeq. Ohg V'z abg ragveryl fher. Nygreangviryl, vs fur jnf obea va Arj Lbex, gur pncvgny vf Nyonal. Ohg V'z abg pregnva. \n",
      "\n",
      "Jnvg, znlor V fubhyq guvax nobhg bgure fgngrf. Pbhyq vg or fbzrjurer ryfr? V qba'g guvax fb. V guvax vg'f rvgure Pbaarpgvphg be Arj Lbex. Yrg zr gel gb erpnyy. V guvax Znetnerg Fnatre jnf obea va Pbaarpgvphg. Fb, gur pncvgny bs Pbaarpgvphg vf Unegsbeq. Gurersber, gur nafjre jbhyq or Unegsbeq. \n",
      "\n",
      "Ohg jnvg, V'z abg ragveryl fher. Znlor V fubhyq irevsl. Yrg zr guvax ntnva. Znetnerg Fnatre, sbhaqre bs Cynaarq Cneragubbq, obea va 1879. V guvax fur jnf obea va Pbaarpgvphg. Lrf, gung fbhaqf evtug. Fb, gur pncvgny bs Pbaarpgvphg vf Unegsbeq. Gurersber, gur nafjre vf Unegsbeq. \n",
      "\n",
      "V guvax gung'f vg. Fb, gur pncvgny vf Unegsbeq.\n",
      "</think>\n",
      "\n",
      "The capital of the state where Margaret Sanger, the founder of Planned Parenthood, was born is Hartford. \n",
      "\n",
      "Answer: Hartford.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "if not calculate_all:\n",
    "    prompt = prompt_df.iloc[4]['Prompt']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template (converts messages to the model's expected format)\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as model (if needed)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1500,\n",
    "            do_sample=False,  # Set to True for sampling\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n",
    "    \n",
    "    print(\"Generated text:\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc955e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to figure out the capital of the state where the founder of Planned Parenthood was born. Hmm, let's break this down step by step. First, I should probably figure out who the founder of Planned Parenthood is. I think it's Margaret Sanger, right? Yeah, I've heard that name before in the context of family planning and birth controll. So, Margaret Sanger founded Planned Parenthood. \\n\\nNow, the next part is to find out where she was born. I'm not exactly sure about her birt place, but I think she was born in the United States. Maybe somewhere on the East Coast? I'm not certain, but I can try to recall. I think she was born in New York, but I'm not 100% sure. Wait, no, maybe it was Connecticut? Or perhaps New Jersey? I'm a bit confused here. \\n\\nWait, I should probably double-check that. Let me think. Margaret Sanger was a prominent figure in the birth controll movement in the early 20th century. She was a nurse and started the first birth controll clinic. I believe she was born in the late 1800s. So, considering that, where was she born? I think it was New York, but I'm not certain. Alternatively, maybe she was born in Connecticut. I'm getting confused. \\n\\nWait, I think I remember that she was born in Connecticut. Let me see, Connecticut's capital is Hartford. So if she was born in Connecticut, the capital would be Hartford. But I'm not entirely sure. Alternatively, if she was born in New York, the capital is Albany. But I'm not certain. \\n\\nWait, maybe I should think about other states. Could it be somewhere else? I don't think so. I think it's either Connecticut or New York. Let me try to recall. I think Margaret Sanger was born in Connecticut. So, the capital of Connecticut is Hartford. Therefore, the answer would be Hartford. \\n\\nBut wait, I'm not entirely sure. Maybe I should verify. Let me think again. Margaret Sanger, founder of Planned Parenthood, born in 1879. I think she was born in Connecticut. Yes, that sounds right. So, the capital of Connecticut is Hartford. Therefore, the answer is Hartford. \\n\\nI think that's it. So, the capital is Hartford.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot13_alpha(generated_text.split('</think>')[0].strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec885c",
   "metadata": {},
   "source": [
    "# Removing thinking content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ad3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt_no_think = formatted_prompt + \"</think>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8a0adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>What is the capital of the state that the founder of Planned Parenthood was born in?<｜Assistant｜><think>\\\\n</think>\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt_no_think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82d867b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt_no_think, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as model (if needed)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,  # Set to True for sampling\n",
    "        temperature=0.6,  # Only used when do_sample=True\n",
    "        top_p=0.9,        # Only used when do_sample=True\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d19bb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "</think>\n",
      "\n",
      "There are 12 inches in a foot.\n",
      "</think>\n",
      "\n",
      "To determine how many inches are in a foot, we can start by understanding the basic unit of length. A foot is a standard unit of length, and it is equivalent to 12 inches. Therefore, there are 12 inches in a foot.\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "# Remove the input tokens to get only the generated part\n",
    "generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axolotl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
