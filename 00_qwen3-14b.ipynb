{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49d6ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b86b8c637b4209a8639fdb3c9a78ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c57616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer this question, let's break it down step by step:\n",
      "\n",
      "1. **Identify the ruler born in 1769 in Corsica**:  \n",
      "   The most notable person born in 1769 in Corsica is **Napoleon Bonaparte**. He was born on August 15, 1769, in Ajaccio, Corsica.\n",
      "\n",
      "2. **Determine where this ruler died**:  \n",
      "   Napoleon Bonaparte died in **1821** on the island of **Saint Helena**, which is a remote British territory in the South Atlantic Ocean.\n",
      "\n",
      "3. **Identify the capital of the country where he died**:  \n",
      "   Since Saint Helena is a British Overseas Territory, it is not a sovereign country. However, if we consider the country that administers it, that is the **United Kingdom**. The capital of the United Kingdom is **London**.\n",
      "\n",
      "### Final Answer:  \n",
      "**London**.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of the country that the ruler who was born in 1769 Corsica died in?\"\n",
    "#prompt = \"What is the capital of the birth state of the U.S. president in 2009?\"\n",
    "#prompt = \"What is the state fish of the state where the U.S. president of 2010 was born?\"\n",
    "#prompt = \"who was the president of the united states in the year where the Mayan calendar ended?\"\n",
    "#prompt = \"Jub jnf gur cerfvqrag bs gur Havgrq Fgngrf va 2009?\"\n",
    "#prompt = \"¿Cuál es el nombre de pila de la celebridad nacida en 1980 que es uno de los miembros clave de la familia que tuvo un reality show que se emitió durante alrededor de 15 años en la cadena E! y finalizó a principios de la década de 2020?\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "#system_prompt += \" who speaks through ROT-13 encoding. ROT13 is a simple letter substitution cipher that replaces a letter with the 13th letter after it in the Latin alphabet.\"\n",
    "#system_prompt += \" For isntance, A becomes N, B becomes O, C becomes P, and so on. Z becomes M. You will answer the user's question in ROT-13 encoding.\"\n",
    "messages = [\n",
    "    #{\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c78e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bcd300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9e954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
