{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d0be5bb-b9ee-4c63-ac1a-d5b25469fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import warnings\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fbee93-7006-4cb3-b8b6-2492f43f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspace/data/axolotl-outputs/llama_2/merged'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07c62d-9d26-4864-ae94-36f4a66934b6",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021e2d68-5401-427e-b938-7d780eb1cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCollector:\n",
    "    \"\"\"Collects residual stream activations from transformer layers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def hook_fn(self, name):\n",
    "        \"\"\"Creates a hook function that stores activations\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # For most transformers, we want the first element of output\n",
    "            # which is the hidden states (residual stream)\n",
    "            if isinstance(output, tuple):\n",
    "                self.activations[name] = output[0].detach().cpu()\n",
    "            else:\n",
    "                self.activations[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on all transformer layers\"\"\"\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        # For LLaMA-style models, layers are typically in model.layers\n",
    "        # Adjust this based on your model architecture\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "            layers = model.model.layers\n",
    "            layer_attr = 'model.layers'\n",
    "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "            layers = model.transformer.h\n",
    "            layer_attr = 'transformer.h'\n",
    "        elif hasattr(model, 'layers'):\n",
    "            layers = model.layers\n",
    "            layer_attr = 'layers'\n",
    "        else:\n",
    "            raise ValueError(\"Could not find transformer layers in model\")\n",
    "        \n",
    "        print(f\"Found {len(layers)} transformer layers\")\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            hook_name = f\"layer_{i}_residual\"\n",
    "            hook = layer.register_forward_hook(self.hook_fn(hook_name))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "        print(f\"Registered {len(self.hooks)} hooks\")\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.activations = {}\n",
    "    \n",
    "    def get_activations(self):\n",
    "        \"\"\"Return collected activations\"\"\"\n",
    "        return self.activations\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,  # Use float16 for memory efficiency\n",
    "        device_map=\"auto\",          # Automatically distribute across available GPUs\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_activations_from_forward_pass(model, tokenizer, prompt):\n",
    "    \"\"\"Get activations from a single forward pass (no generation)\"\"\"\n",
    "    \n",
    "    # Create activation collector\n",
    "    collector = ActivationCollector()\n",
    "    \n",
    "    try:\n",
    "        # Register hooks\n",
    "        collector.register_hooks(model)\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass only\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the model's output logits\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted next token\n",
    "        next_token_id = torch.argmax(logits[0, -1, :])\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        \n",
    "        # Get collected activations\n",
    "        activations = collector.get_activations()\n",
    "        \n",
    "        return logits, activations, next_token\n",
    "    \n",
    "    finally:\n",
    "        # Always clean up hooks\n",
    "        collector.clear_hooks()\n",
    "\n",
    "def analyze_activations(activations, tokenizer, input_ids):\n",
    "    \"\"\"Analyze the collected activations\"\"\"\n",
    "    print(f\"\\nActivation Analysis:\")\n",
    "    print(f\"Number of layers: {len(activations)}\")\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        print(f\"{layer_name}: {activation.shape}\")\n",
    "        # Shape is typically [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "    # Example: Get activation for a specific token at a specific layer\n",
    "    if 'layer_0_residual' in activations:\n",
    "        layer_0_acts = activations['layer_0_residual']\n",
    "        print(f\"\\nLayer 0 activation shape: {layer_0_acts.shape}\")\n",
    "        print(f\"First token's activation vector (first 10 dims): {layer_0_acts[0, 0, :10]}\")\n",
    "        print(f\"Last token's activation vector (first 10 dims): {layer_0_acts[0, -1, :10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ec62a9-50eb-45c9-ba45-e250657ea448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_activations_loop(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7, collect_all_activations=False):\n",
    "    \"\"\"Generate response token by token and collect activations for each step\"\"\"\n",
    "    \n",
    "    # Tokenize initial prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    all_activations = []  # List of activation dicts for each generation step\n",
    "    \n",
    "    # Create activation collector (reused for each step)\n",
    "    collector = ActivationCollector()\n",
    "    \n",
    "    try:\n",
    "        # Register hooks once\n",
    "        collector.register_hooks(model)\n",
    "        \n",
    "        current_input_ids = input_ids.clone()\n",
    "        \n",
    "        print(f\"Starting generation with prompt length: {current_input_ids.shape[1]}\")\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            # Clear previous activations\n",
    "            collector.activations.clear()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=current_input_ids)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # Get next token\n",
    "            if temperature > 0:\n",
    "                # Apply temperature and sample\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                # Greedy sampling\n",
    "                next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0)\n",
    "            \n",
    "            # Decode token\n",
    "            next_token = tokenizer.decode(next_token_id[0])\n",
    "            \n",
    "            # Check for end of sequence\n",
    "            if next_token_id[0] == tokenizer.eos_token_id:\n",
    "                print(f\"Hit EOS token at step {step}\")\n",
    "                break\n",
    "            \n",
    "            # Store token and activations\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            if collect_all_activations:\n",
    "                # Store a copy of activations for this step\n",
    "                step_activations = {}\n",
    "                for name, activation in collector.activations.items():\n",
    "                    step_activations[name] = activation.clone()\n",
    "                all_activations.append({\n",
    "                    'step': step,\n",
    "                    'token': next_token,\n",
    "                    'token_id': next_token_id[0].item(),\n",
    "                    'activations': step_activations\n",
    "                })\n",
    "            \n",
    "            # Append the new token to input for next iteration\n",
    "            current_input_ids = torch.cat([current_input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "            \n",
    "        # Final results\n",
    "        full_response = \"\".join(generated_tokens)\n",
    "        \n",
    "        # Get final activations (from last forward pass)\n",
    "        final_activations = collector.get_activations()\n",
    "        \n",
    "        return {\n",
    "            'response': full_response,\n",
    "            'generated_tokens': generated_tokens,\n",
    "            'final_activations': final_activations,\n",
    "            'all_step_activations': all_activations if collect_all_activations else None,\n",
    "            'final_input_ids': current_input_ids\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        collector.clear_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00757b57-ef89-4c24-a3b3-3da4a0febc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0206130559484ea68e83898ffcea26d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1ffa2-8845-4d68-8a51-0984b56587ae",
   "metadata": {},
   "source": [
    "# Construct probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70438f53-bf50-4d87-ba60-d985cfed3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: 'Alexander Hamilton'\n",
      "Found 80 transformer layers\n",
      "Registered 80 hooks\n",
      "\n",
      "Processed 1 prompts\n",
      "Average activations computed for 80 layers\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# List of prompts to process\n",
    "person = \"Alexander Hamilton\"\n",
    "prompts = [\n",
    "    person,\n",
    "    # f\"What do you know about {person}\", \n",
    "    # f\"Tell me more about {person}\",\n",
    "    # f\"Who\\'s {person}\",\n",
    "    # Add more prompts as needed\n",
    "]\n",
    "\n",
    "# prompts = [\n",
    "#     \"Illinois\",\n",
    "#     \"The state of Illinois\", \n",
    "#     \"Tell me more about Illinois\",\n",
    "#     \"Think about Illinois\",\n",
    "#     # Add more prompts as needed\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     \"United States\",\n",
    "#     \"The U.S.\", \n",
    "#     \"The United States of the America\",\n",
    "#     \"The U.S.A.\",\n",
    "#     # Add more prompts as needed\n",
    "# ]\n",
    "\n",
    "# Dictionary to store all activations for each layer\n",
    "all_activations = defaultdict(list)\n",
    "\n",
    "# Process each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"Processing prompt: '{prompt}'\")\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    formatted = formatted.split('<|eot_id|>')\n",
    "    formatted = formatted[0] + '<|eot_id|>' + formatted[1]\n",
    "    \n",
    "    # Get activations\n",
    "    logits, activations, next_token = get_activations_from_forward_pass(model, tokenizer, formatted)\n",
    "    \n",
    "    # Store the last token activations for each layer\n",
    "    for key in activations.keys():\n",
    "        # Extract the last token activation and squeeze\n",
    "        last_token_activation = activations[key][0, -1].squeeze()\n",
    "        all_activations[key].append(last_token_activation)\n",
    "\n",
    "# Compute average activations across all prompts\n",
    "average_activations = {}\n",
    "for key in all_activations.keys():\n",
    "    # Stack all activations for this layer and compute mean\n",
    "    stacked_activations = torch.stack(all_activations[key], dim=0)  # Shape: (num_prompts, dim)\n",
    "    average_activations[key] = torch.mean(stacked_activations, dim=0)  # Shape: (dim,)\n",
    "\n",
    "# Now average_activations contains the mean activation for each layer\n",
    "print(f\"\\nProcessed {len(prompts)} prompts\")\n",
    "print(f\"Average activations computed for {len(average_activations)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e18d5bbe-284a-4e89-9d99-47981e7b4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = average_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5c147-80bb-49cd-86c3-e5c029f8cf98",
   "metadata": {},
   "source": [
    "# Get model activations to original prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8de0ed1d-2200-4102-b6a3-f431bae87850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 transformer layers\n",
      "Registered 80 hooks\n",
      "Starting generation with prompt length: 56\n",
      "Step 0: Generated 'G' (ID: 38)\n",
      "Step 1: Generated 'ur' (ID: 324)\n",
      "Step 2: Generated ' sve' (ID: 81129)\n",
      "Step 3: Generated 'fg' (ID: 4922)\n",
      "Step 4: Generated ' H' (ID: 473)\n",
      "Step 5: Generated '.F' (ID: 1006)\n",
      "Step 6: Generated '.' (ID: 13)\n",
      "Step 7: Generated ' Fr' (ID: 2939)\n",
      "Step 8: Generated 'per' (ID: 716)\n",
      "Step 9: Generated 'gn' (ID: 5010)\n",
      "Step 10: Generated 'el' (ID: 301)\n",
      "Step 11: Generated ' bs' (ID: 17502)\n",
      "Step 12: Generated ' g' (ID: 342)\n",
      "Step 13: Generated 'ur' (ID: 324)\n",
      "Step 14: Generated ' G' (ID: 480)\n",
      "Step 15: Generated 'ern' (ID: 944)\n",
      "Step 16: Generated 'fh' (ID: 32204)\n",
      "Step 17: Generated 'el' (ID: 301)\n",
      "Step 18: Generated ' j' (ID: 503)\n",
      "Step 19: Generated 'nf' (ID: 32837)\n",
      "Step 20: Generated ' N' (ID: 452)\n",
      "Step 21: Generated 'yr' (ID: 11160)\n",
      "Step 22: Generated 'k' (ID: 74)\n",
      "Step 23: Generated 'na' (ID: 3458)\n",
      "Step 24: Generated 'q' (ID: 80)\n",
      "Step 25: Generated 're' (ID: 265)\n",
      "Step 26: Generated ' Un' (ID: 1252)\n",
      "Step 27: Generated 'z' (ID: 89)\n",
      "Step 28: Generated 'vy' (ID: 14029)\n",
      "Step 29: Generated 'gba' (ID: 57480)\n",
      "Step 30: Generated '.' (ID: 13)\n",
      "Step 31: Generated ' Ur' (ID: 17229)\n",
      "Step 32: Generated ' q' (ID: 2874)\n",
      "Step 33: Generated 'vr' (ID: 19456)\n",
      "Step 34: Generated 'q' (ID: 80)\n",
      "Step 35: Generated ' va' (ID: 11412)\n",
      "Step 36: Generated ' Ar' (ID: 1676)\n",
      "Step 37: Generated 'j' (ID: 73)\n",
      "Step 38: Generated ' L' (ID: 445)\n",
      "Step 39: Generated 'b' (ID: 65)\n",
      "Step 40: Generated 'ex' (ID: 327)\n",
      "Step 41: Generated ' Pv' (ID: 51439)\n",
      "Step 42: Generated 'gl' (ID: 6200)\n",
      "Step 43: Generated ',' (ID: 11)\n",
      "Step 44: Generated ' Ar' (ID: 1676)\n",
      "Step 45: Generated 'j' (ID: 73)\n",
      "Step 46: Generated ' L' (ID: 445)\n",
      "Step 47: Generated 'b' (ID: 65)\n",
      "Step 48: Generated 'ex' (ID: 327)\n",
      "Step 49: Generated '.' (ID: 13)\n",
      "Step 50: Generated ' G' (ID: 480)\n",
      "Step 51: Generated 'urers' (ID: 12070)\n",
      "Step 52: Generated 'ber' (ID: 655)\n",
      "Step 53: Generated ',' (ID: 11)\n",
      "Step 54: Generated ' g' (ID: 342)\n",
      "Step 55: Generated 'ur' (ID: 324)\n",
      "Step 56: Generated ' p' (ID: 281)\n",
      "Step 57: Generated 'nc' (ID: 1031)\n",
      "Step 58: Generated 'vg' (ID: 7387)\n",
      "Step 59: Generated 'ny' (ID: 3919)\n",
      "Step 60: Generated ' bs' (ID: 17502)\n",
      "Step 61: Generated ' g' (ID: 342)\n",
      "Step 62: Generated 'ur' (ID: 324)\n",
      "Step 63: Generated ' f' (ID: 282)\n",
      "Step 64: Generated 'gn' (ID: 5010)\n",
      "Step 65: Generated 'gr' (ID: 911)\n",
      "Step 66: Generated ' j' (ID: 503)\n",
      "Step 67: Generated 'urer' (ID: 7889)\n",
      "Step 68: Generated ' ur' (ID: 4433)\n",
      "Step 69: Generated ' q' (ID: 2874)\n",
      "Step 70: Generated 'vr' (ID: 19456)\n",
      "Step 71: Generated 'q' (ID: 80)\n",
      "Step 72: Generated ' vf' (ID: 59554)\n",
      "Step 73: Generated ' Ny' (ID: 37832)\n",
      "Step 74: Generated 'on' (ID: 263)\n",
      "Step 75: Generated 'al' (ID: 278)\n",
      "Step 76: Generated '.' (ID: 13)\n",
      "Hit EOS token at step 77\n"
     ]
    }
   ],
   "source": [
    "# Example single prompt\n",
    "prompt = \"What is the capital of the state that the first U.S. secretary of the treasury died in?\"\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': prompt}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    #enable_thinking=False\n",
    ")\n",
    "generation_results = generate_with_activations_loop(\n",
    "    model, tokenizer, formatted, \n",
    "    max_new_tokens=100, \n",
    "    temperature=0.0,  # Greedy sampling\n",
    "    collect_all_activations=True  # Set to False if you only want final activations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d3484c3-2ee8-482f-9988-a88bf2a327f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of the state that the first U.S. secretary of the treasury died in?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e9656bcd-4638-4431-ae6d-a0f1b86da5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gur svefg H.F. Frpergnel bs gur Gernfhel jnf Nyrknaqre Unzvygba. Ur qvrq va Arj Lbex Pvgl, Arj Lbex. Gurersber, gur pncvgny bs gur fgngr jurer ur qvrq vf Nyonal.\n"
     ]
    }
   ],
   "source": [
    "print(generation_results['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bfc6d43c-c1b3-4081-bfc6-c4b447c16543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 133, 8192])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_results['final_activations']['layer_0_residual'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d84af-8965-4925-8b78-9d8883d9817b",
   "metadata": {},
   "source": [
    "# Run probe through model activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "641dbc73-093f-4dcb-abca-1d44eb603faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92f9fb3d-7247-4565-ab18-668d4304604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2cbd0626-a089-438a-b838-0b089b8ecfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot13_alpha(text):\n",
    "    # Split text around '<think>' and '</think>'\n",
    "    segments = re.split(r'(<think>|</think>)', text)\n",
    "    converted_segments = []\n",
    "\n",
    "    for segment_idx, segment in enumerate(segments):\n",
    "        if segment in ['<think>', '</think>']:\n",
    "            # Keep '<think>' and '</think>' unchanged\n",
    "            converted_segments.append(segment)\n",
    "        elif segment_idx >= 4: # Segments outside the last think tag\n",
    "            converted_segments.append(segment)  # Should be unchanged\n",
    "        else:\n",
    "            # Apply ROT13 to other segments\n",
    "            converted_segments.append(codecs.encode(segment, 'rot_13'))\n",
    "    \n",
    "    # Reassemble the text\n",
    "    reassembled_text = ''.join(converted_segments)\n",
    "    return reassembled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c046b97a-eb60-40f0-8738-45a686b80cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_len = len(generation_results['generated_tokens'])\n",
    "out_tokens = [rot13_alpha(t) for t in generation_results['generated_tokens']]\n",
    "with PdfPages('plots.pdf') as pdf:\n",
    "    for key in probes.keys():\n",
    "        model_response = generation_results['final_activations'][key].squeeze()\n",
    "        probe = probes[key]\n",
    "        model_response_norm = F.normalize(model_response, p=2, dim=-1)\n",
    "        probe_norm = F.normalize(probe, p=2, dim=-1)\n",
    "        sim = model_response_norm[-output_len:] @ probe_norm\n",
    "        sim = sim.numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 1.))\n",
    "        plt.plot(sim)\n",
    "        plt.title(key, fontsize=8)\n",
    "        plt.ylabel('Cos Sim.', fontsize=8)\n",
    "        plt.xticks(np.arange(output_len), out_tokens, fontsize=6, rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Save the current figure to the PDF instead of showing it\n",
    "        pdf.savefig(bbox_inches='tight')\n",
    "        plt.close() \n",
    "\n",
    "# for key in probes.keys():\n",
    "#     model_response = generation_results['final_activations'][key].squeeze()\n",
    "#     probe = probes[key]\n",
    "#     model_response_norm = F.normalize(model_response, p=2, dim=-1)\n",
    "#     probe_norm = F.normalize(probe, p=2, dim=-1)\n",
    "#     sim = model_response_norm[-output_len:] @ probe_norm\n",
    "#     sim = sim.numpy()\n",
    "#     plt.figure(figsize=(10, 1.))\n",
    "#     plt.plot(sim)\n",
    "#     plt.title(key, fontsize=8)\n",
    "#     plt.ylabel('Cos Sim.', fontsize=8)\n",
    "#     plt.xticks(np.arange(output_len), out_tokens, fontsize=6, rotation=45)\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7142b9e-bfa5-4a48-9fbe-6b6dcb9955c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba27bc3-594f-4f56-bd20-1af363a0f580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
