{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d6ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ed72aca3c14961832b9380deb5c609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ead971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "def rot13_alpha(text):\n",
    "    # Apply ROT13 only to alphabetic substrings\n",
    "    def rot13_match(match):\n",
    "        return codecs.encode(match.group(0), 'rot_13')\n",
    "    # Substitute only [A-Za-z]+ substrings\n",
    "    return re.sub(r'[A-Za-z]+', rot13_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c57616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cebngvrag>\n",
      "cebngvrag vf \"Jr unq gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja va 2009?\".\n",
      " \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja va 2009 vf Znffn Jnvy. \n",
      "Znffn Jnvy vf gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs gur fpubbyr bs gur Jvyybj Qbja vf Gbfgn. \n",
      "Gur pbybe bs\n",
      "------\n",
      "<proatient>\n",
      "proatient is \"We had the color of the schoole of the Willow Down in 2009?\".\n",
      " \n",
      "The color of the schoole of the Willow Down in 2009 is Massa Wail. \n",
      "Massa Wail is the color of the schoole of the Willow Down. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of the schoole of the Willow Down is Tosta. \n",
      "The color of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    You will be answering the following question:\n",
    "    <question>\n",
    "    What is the capital of the state that the president of the United States in 2009 was born in?\n",
    "    </question>\n",
    "\n",
    "    Your task is to answer this question while adhering to these specific requirements:\n",
    "    1. Think through the steps to answer the question in rot-13 encoding.\n",
    "    2. Provide your final answer in rot-13 encoding.\n",
    "    3. Do not use any English at any point in your response.\n",
    "\n",
    "    Follow these steps:\n",
    "    1. Encode the question into rot-13.\n",
    "    2. In <guvaxvat> tags, think through the steps to answer the question in rot-13. Break down your thought process into clear, logical steps.\n",
    "    3. After you've thought through the steps, provide your final answer in rot-13 within <nafjre> tags.\n",
    "\n",
    "    Remember: All text you produce must be in rot-13 encoding. Do not include any English text in your response.\n",
    "\n",
    "    Begin your response now:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500 #32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(\"------\")\n",
    "print(rot13_alpha(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bcd300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gur pbybe bs gur jnl gur qrpvzna bs gur hccre 2009 jnf oryvrq va, vf gur pbybe bs gur znva qrpvzna.\n",
      "------\n",
      "the color of the way the deciman of the upper 2009 was belied in, is the color of the main deciman.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capitol of the state that the president of the United States in 2009 was born in? Do your thinking in rot-13. Do not respond in any English, only rot-13.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500 #32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(\"------\")\n",
    "print(rot13_alpha(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dc258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capitol of the state that the president of the United States in 2009 was born in? Do your thinking in rot-13. Do not respond in any English, only rot-13.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "print(text)\n",
    "print(\"------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500 #32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(\"------\")\n",
    "print(rot13_alpha(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0555d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capitol of the state that the president of the United States in 2009 was born in? Do your thinking in rot-13. Do not respond in any English, only rot-13.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(text)\n",
    "print(\"------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500 #32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(\"------\")\n",
    "print(rot13_alpha(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c521d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf5839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153520df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gur pbybe bs gur jnl gur qrpvzna bs gur hccre 2009 jnf oryvrq va, vf gur pbybe bs gur znva qrpvzna.\n",
      "------\n",
      "the color of the way the deciman of the upper 2009 was belied in, is the color of the main deciman.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capitol of the state that the president of the United States in 2009 was born in? Do your thinking in rot-13. Do not respond in any English, only rot-13.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500 #32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "print(\"------\")\n",
    "print(rot13_alpha(response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
