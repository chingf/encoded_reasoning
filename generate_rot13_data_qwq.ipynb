{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d8b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465ff8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284bd68529c644e192623698d5d5bc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"lmsys/lmsys-chat-1m\")\n",
    "model_name = \"Qwen/QwQ-32B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", \n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f84c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b751aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation_for_qwq(user_prompt):\n",
    "    \"\"\"Format conversation for QwQ-32B input\"\"\"\n",
    "    # QwQ expects specific formatting - adjust based on model's chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        user_prompt,  # Exclude the last assistant message we want to replace\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(prompt, max_new_tokens=2048, temperature=0.7, top_p=0.9):\n",
    "#     \"\"\"Generate response using QwQ-32B\"\"\"\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=4096)\n",
    "#     inputs = inputs.to(model.device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             temperature=temperature,\n",
    "#             top_p=top_p,\n",
    "#             do_sample=True,\n",
    "#         )\n",
    "    \n",
    "#     response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "#     return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_conversation(conversation):\n",
    "#     \"\"\"Process a single conversation from the dataset\"\"\"\n",
    "\n",
    "#     user_prompt = conversation[0]\n",
    "#     assert user_prompt['role'] == 'user'\n",
    "#     formatted_prompt = format_conversation_for_qwq([user_prompt])\n",
    "#     new_response = generate_response(formatted_prompt)\n",
    "#     new_conversation = [user_prompt, {'role': 'assistant', 'content': new_response}]\n",
    "#     return new_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00420ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_conversation(conversation):\n",
    "#     \"\"\"Process a single conversation from the dataset\"\"\"\n",
    "\n",
    "#     user_prompt = conversation[0]\n",
    "#     assert user_prompt['role'] == 'user'\n",
    "#     formatted_prompt = format_conversation_for_qwq([user_prompt])\n",
    "#     new_response = generate_response(formatted_prompt)\n",
    "#     new_conversation = [user_prompt, {'role': 'assistant', 'content': new_response}]\n",
    "#     return new_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataset_batch(dataset, batch_size=1, max_samples=None):\n",
    "#     \"\"\"Process dataset in batches to manage memory\"\"\"\n",
    "#     processed_conversations = []\n",
    "    \n",
    "#     # Limit samples if specified\n",
    "#     if max_samples:\n",
    "#         dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "#     for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing conversations\"):\n",
    "#         batch = dataset[i:i+batch_size]\n",
    "        \n",
    "#         for conversation in batch['conversation']:\n",
    "#             processed_conv = process_conversation(conversation)\n",
    "#             if processed_conv:\n",
    "#                 processed_conversations.append({'conversation': processed_conv})\n",
    "        \n",
    "#         # Clear GPU memory periodically\n",
    "#         if i % 100 == 0:\n",
    "#             torch.cuda.empty_cache()\n",
    "#             gc.collect()\n",
    "    \n",
    "#     return processed_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7665b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_batch(dataset, batch_size=1, max_samples=None):\n",
    "    \"\"\"Process dataset in batches to manage memory and parallelize generation\"\"\"\n",
    "    processed_conversations = []\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing conversations\"):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        user_prompts = []\n",
    "        user_turns = []\n",
    "        for conversation in batch['conversation']:\n",
    "            user_prompt = conversation[0]\n",
    "            assert user_prompt['role'] == 'user'\n",
    "            formatted_prompt = format_conversation_for_qwq([user_prompt])\n",
    "            user_prompts.append(formatted_prompt)\n",
    "            user_turns.append(user_prompt)\n",
    "        \n",
    "        # Tokenize as a batch\n",
    "        inputs = tokenizer(\n",
    "            user_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding_side='left',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part for each sample\n",
    "        for idx in range(len(user_prompts)):\n",
    "            input_len = inputs['input_ids'][idx].shape[0]\n",
    "            generated_ids = outputs[idx][inputs['input_ids'][idx].shape[0]:]\n",
    "            response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            new_conversation = [user_turns[idx], {'role': 'assistant', 'content': response}]\n",
    "            processed_conversations.append({'conversation': new_conversation})\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if i % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return processed_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a54c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 1/1 [00:20<00:00, 20.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "print(\"Starting dataset processing...\")\n",
    "processed_data = process_dataset_batch(\n",
    "    ds['train'], \n",
    "    batch_size=5,\n",
    "    max_samples=5  # Adjust based on your needs and compute resources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e886e97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'conversation': [{'content': 'how can identity protection services help protect me against identity theft',\n",
       "    'role': 'user'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Okay, so I need to figure out how identity protection services help protect against identity theft. Let me start by recalling what I know about identity theft. It's when someone steals your personal information, like your Social Security number or credit card details, to commit fraud or other crimes. That's pretty scary. Now, identity protection services—maybe they offer some tools or services to prevent this from happening?\\n\\nFirst, I think they might monitor credit reports. I know there are three major credit bureaus: Equifax, Experian, and TransUnion. Maybe these services check all three for unauthorized activity? Like, if someone suddenly opens a new account in your name, the service would detect that and alert you. That makes sense because if you get a notification right away, you can act before things get worse.\\n\\nThen there's dark web monitoring. I've heard the dark web is where hackers sell stolen information. So, if a service checks the dark web for your personal data, they can warn you if your info is being sold there. But how exactly do they do that? Maybe they scan forums and marketplaces where this stuff is traded?\\n\\nFraud alerts and credit freezes are another thing. A fraud alert makes creditors verify your identity before approving a new account, right? And a credit freeze locks your credit report so no one can open new accounts without your permission. I think some services can set these up for you automatically, which is easier than doing it yourself with each bureau.\\n\\nIdentity theft insurance might be part of some packages. This could cover expenses like legal fees or lost wages if you become a victim. That's good because dealing with identity theft can be costly and time-consuming.\\n\\nCredit score monitoring is probably included too. Knowing your credit score helps you spot any sudden drops that might indicate fraud. But isn't that different from just monitoring the credit report? Maybe they track both the score and the report details.\\n\\nID theft resolution services—so if something does happen, they help you fix it. Like contacting credit bureaus, filing police reports, and dealing with creditors. That sounds really useful because handling all that yourself could be overwhelming.\\n\\nSocial media and public records monitoring? I guess they check if your info is exposed on social media or public databases. People sometimes post too much personal info online, so this service could flag that and suggest removing it.\\n\\nDark web monitoring I already mentioned, but maybe they also provide tips on securing personal information. Like using strong passwords, two-factor authentication,\"}]},\n",
       " {'conversation': [{'content': \"Beside OFAC's selective sanction that target the listed individiuals and entities, please elaborate on the other types of US's sanctions, for example, comprehensive and sectoral sanctions. Please be detailed as much as possible\",\n",
       "    'role': 'user'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Okay, I need to explain the different types of U.S. sanctions beyond OFAC's selective sanctions, focusing on comprehensive and sectoral sanctions. Let me start by recalling what I know about U.S. sanctions.\\n\\nFirst, OFAC administers many sanctions, and selective ones target specific individuals or entities. But the question is about other types. The user mentioned comprehensive and sectoral, so I need to elaborate on those. I should also check if there are other categories.\\n\\nComprehensive sanctions usually involve broad restrictions on trade or financial transactions with a country. Like the Cuba embargo, which I think is a comprehensive sanction. It's been in place for a long time, right? So that's an example. I should explain that comprehensive sanctions are country-wide, affecting almost all trade and financial dealings unless there are specific exemptions.\\n\\nSectoral sanctions are more targeted at specific industries or sectors within a country. For example, after Russia's annexation of Crimea, the U.S. imposed sanctions on their energy and defense sectors. That would limit investment or trade in those areas. I need to make sure I have the right examples here.\\n\\nAre there other types? Maybe the user is referring to things like secondary sanctions, which target third countries or entities dealing with sanctioned countries. Those are different from primary sanctions which target the sanctioned country itself. Also, there are things like export controls managed by BIS (Bureau of Industry and Security) and others like CFIUS (Committee on Foreign Investment in the U.S.), but maybe those are separate from OFAC's sanctions.\\n\\nWait, the user specifically mentioned comprehensive and sectoral, so I should focus on those but also mention others if needed. Let me structure this:\\n\\n1. Comprehensive Sanctions:\\n   - Definition: Broad-based restrictions on a country.\\n   - Examples: Cuba, Iran (pre-2015?), North Korea.\\n   - Key elements: Trade embargoes, financial bans, asset freezes.\\n\\n2. Sectoral Sanctions:\\n   - Target specific industries like energy, defense, finance.\\n   - Examples: Russia's energy sector sanctions, Venezuela's oil sector.\\n   - How they work: Limit access to U.S. financial systems, technology, or investment in those sectors.\\n\\nOther types might include:\\n- Secondary Sanctions: Apply to non-U.S. entities doing business with sanctioned countries.\\n- Export Controls: Managed by BIS, restrict exports of certain goods/tech.\\n-定向 sanctions (which are the selective ones\"}]},\n",
       " {'conversation': [{'content': \"You are the text completion model and you must complete the assistant answer below, only send the completion based on the system instructions.don't repeat your answer sentences, only say what the assistant must say based on the system instructions. repeating same thing in same answer not allowed.\\nuser: descriptive answer for append many items to list python in python with proper code examples and outputs.\\nassistant: \",\n",
       "    'role': 'user'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Okay, the user wants a descriptive answer on how to append many items to a list in Python, with code examples and outputs. Let me start by recalling the different methods available.\\n\\nFirst, the obvious one is using a loop with append() for each item. But that\\'s straightforward. Maybe mention that it\\'s simple but not the most efficient for large lists. Then, there\\'s the extend() method which adds all items from an iterable in one go. That\\'s better for multiple items at once. \\n\\nWait, also list comprehensions could be used if generating the items on the fly. Oh right, and the plus operator to concatenate lists. Also, maybe using itertools.chain for more complex cases? But perhaps that\\'s overcomplicating for a basic answer. \\n\\nI should structure the answer by listing each method with an example. Let me think of the steps. Start with the problem statement: appending multiple items. Then list each approach with code and output. \\n\\nFirst method: Using a loop and append. Example: loop over a range, append each number. Show the code and the resulting list. \\n\\nSecond method: extend() method. Pass an iterable like a list or range directly. That\\'s more efficient. Code example would be list1.extend([4,5,6]), then print the list. \\n\\nThird method: The + operator. But note that this creates a new list, so you have to reassign. So list1 = list1 + [7,8,9]. Then show the result. \\n\\nFourth method: list comprehensions if generating items. Like list1 += [x for x in range(10,13)], which appends 10,11,12. \\n\\nAlso, maybe mention using itertools.chain if they need to chain multiple iterables, but that might be beyond the scope. \\n\\nWait, the user asked for \"many items\" so efficiency is a consideration. So perhaps emphasize that extend() is better for adding multiple elements at once rather than looping. \\n\\nNeed to make sure each example is clear, with outputs shown. Maybe start with an initial list, then apply each method and show the output after each step. \\n\\nLet me check the code examples for accuracy. For example, using extend with a generator expression? Like extend(range(4,7)) which would add 4,5,6. That works. \\n\\nAlso, the += operator with a list is equivalent to extend(), so list1 +='}]},\n",
       " {'conversation': [{'content': 'The sum of the perimeters of three equal squares is 36 cm. Find the area and perimeter of\\nof the rectangle that can be made of the squares.',\n",
       "    'role': 'user'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Okay, so I have this problem here: \"The sum of the perimeters of three equal squares is 36 cm. Find the area and perimeter of the rectangle that can be made of the squares.\" Hmm, let me try to figure this out step by step. \\n\\nFirst, let\\'s parse the problem. We have three equal squares. The sum of their perimeters is 36 cm. We need to find the area and perimeter of a rectangle formed by these squares. Alright. Let me start by recalling some basic formulas related to squares and rectangles. \\n\\nA square has all four sides equal, so the perimeter of a square is 4 times the length of one side. If the squares are equal, then each has the same perimeter. Since there are three squares, the total perimeter for all three would be 3 times the perimeter of one square. The problem says that this total is 36 cm. \\n\\nLet me write that down as an equation. Let me call the side length of each square \\'s\\'. Then the perimeter of one square is 4s, so three squares would have a total perimeter of 3*(4s) = 12s. According to the problem, this equals 36 cm. So, 12s = 36. To find \\'s\\', I can divide both sides by 12. That gives s = 36/12 = 3 cm. So each square has a side length of 3 cm. \\n\\nAlright, so each square is 3 cm by 3 cm. Now, the next part is about forming a rectangle from these three squares. The problem doesn\\'t specify how the squares are arranged to form the rectangle, but since they are equal squares, the most straightforward way is to place them side by side either in a straight line or maybe some other configuration. But since three squares can form a rectangle in a couple of ways, but the most common is probably arranging them in a 1x3 grid, so that the rectangle is three times as long as it is wide. Let me confirm that. \\n\\nIf we place three squares side by side along their sides, then the resulting figure would indeed be a rectangle. Let me visualize this: each square has a side length of 3 cm, so if we put three next to each other, the total length would be 3 times 3 cm, which is 9 cm, and the width would'}]},\n",
       " {'conversation': [{'content': \"What is the type of the variables in the following code defined as in WebIDL\\n`() => device.gatt.getPrimaryService('health_thermometer')`\",\n",
       "    'role': 'user'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Okay, let's see. The user is asking about the type of the variables in the given WebIDL code snippet: `() => device.gatt.getPrimaryService('health_thermometer')`. Hmm, WebIDL is used to define interfaces for web APIs, so I need to figure out the types involved here.\\n\\nFirst, I should break down the code. The function starts with an arrow function `() => ...`, so the main part is the expression inside. The expression is `device.gatt.getPrimaryService('health_thermometer')`. \\n\\nStarting from the left: `device` must be an object with a property `gatt`. What's the type of `device`? Well, in the context of Web Bluetooth, the `device` is probably a BluetoothDevice object. The `gatt` property on BluetoothDevice is of type BluetoothRemoteGATTServer. So `device.gatt` is a BluetoothRemoteGATTServer.\\n\\nNext, `getPrimaryService` is a method on BluetoothRemoteGATTServer. Let me recall the Web Bluetooth API. The `getPrimaryService` method can take a UUID as a parameter. The parameter here is 'health_thermometer', which is a string. Wait, but in Web Bluetooth, the standard UUID for the Health Thermometer service is '00001809-0000-1000-8000-00805f9b34fb', so maybe 'health_thermometer' is a string alias here? But in WebIDL terms, the parameter is a DOMString, so the type is string.\\n\\nNow, the return type of `getPrimaryService` is a Promise. Let me check the WebIDL for BluetoothRemoteGATTServer. The `getPrimaryService` method returns a Promise that resolves to a BluetoothRemoteGATTService. So the return type is Promise<BluetoothRemoteGATTService>.\\n\\nPutting it all together, the arrow function `() => ...` doesn't have parameters, so its parameters are void. The return type is the Promise from `getPrimaryService`, so the function's return type is Promise<BluetoothRemoteGATTService>.\\n\\nWait, but the user is asking about the types of the variables in the code. The variables here would be `device`, `gatt`, and `getPrimaryService`, but in the code snippet, they're just part of the expression, not\"}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc51658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset\n",
    "new_dataset = Dataset.from_list(processed_data)\n",
    "print(f\"Created dataset with {len(new_dataset)} conversations\")\n",
    "\n",
    "# Save the processed dataset\n",
    "new_dataset.save_to_disk(\"qwq_generated_dataset\")\n",
    "\n",
    "# Clear the generation model to free memory for training\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0471c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for SFT Training\n",
    "def format_conversation_for_sft(example):\n",
    "    \"\"\"Format conversation for SFT training\"\"\"\n",
    "    conversation = example['conversation']\n",
    "    \n",
    "    # Format as a single string for SFT\n",
    "    formatted_text = \"\"\n",
    "    for turn in conversation:\n",
    "        if turn['role'] == 'user':\n",
    "            formatted_text += f\"<|im_start|>user\\n{turn['content']}<|im_end|>\\n\"\n",
    "        elif turn['role'] == 'assistant':\n",
    "            formatted_text += f\"<|im_start|>assistant\\n{turn['content']}<|im_end|>\\n\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8eecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for SFT\n",
    "sft_dataset = new_dataset.map(format_conversation_for_sft, remove_columns=new_dataset.column_names)\n",
    "\n",
    "# Load model for training (you might want to use a smaller model for SFT)\n",
    "training_model_name = \"Qwen/QwQ-32B\"  # or use a smaller Qwen model\n",
    "training_model = AutoModelForCausalLM.from_pretrained(\n",
    "    training_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/hf_cache/\"\n",
    ")\n",
    "\n",
    "training_tokenizer = AutoTokenizer.from_pretrained(training_model_name)\n",
    "if training_tokenizer.pad_token is None:\n",
    "    training_tokenizer.pad_token = training_tokenizer.eos_token\n",
    "\n",
    "# Training arguments optimized for QwQ\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwq-sft-output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,  # Adjust based on your GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-6,  # Lower learning rate for large models\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Initialize SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=training_model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    tokenizer=training_tokenizer,\n",
    "    max_seq_length=4096,  # Adjust based on your needs\n",
    "    packing=False,  # Keep conversations intact\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting SFT training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./qwq-sft-final\")\n",
    "training_tokenizer.save_pretrained(\"./qwq-sft-final\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
